{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import linguamind.linalg as la\n",
    "import linguamind.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \n",
    "        self.sparse_output = False\n",
    "        self.sparse_input = False\n",
    "        \n",
    "        self.input_dim = input_dim # embedding dim\n",
    "        self.output_dim = output_dim # output vocab size \n",
    "        \n",
    "        self.weights = la.Matrix(output_dim,input_dim)\n",
    "        \n",
    "        self.output = la.Vector(output_dim).zero()\n",
    "        self.input_grad = la.Vector(self.input_dim).zero()\n",
    "        \n",
    "        self.full_output_indices = list(range(self.output_dim))\n",
    "    \n",
    "    def updateOutput(self, input, output_indices=None):\n",
    "        for index in range(self.output_dim):\n",
    "            self.output.doti(index,input, self.weights[index])\n",
    "    \n",
    "    def updateInputGrad(self, output_grad):\n",
    "        self.input_grad.set(self.weights[0],output_grad[0]) \n",
    "        for index in range(self.output_dim-1):\n",
    "            self.input_grad.addi(self.weights[index+1],output_grad[index+1])\n",
    "            \n",
    "    def accGradParameters(self, input, output_grad, alpha):\n",
    "        for index in range(self.output_dim):\n",
    "            self.weights[index].addi(input,output_grad[index] * -alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SparseLinearInput():\n",
    "    \n",
    "    def __init__(self,input_dim, output_dim):\n",
    "        \n",
    "        self.sparse_output = False\n",
    "        self.sparse_input = True\n",
    "        \n",
    "        self.input_dim = input_dim # vocab size\n",
    "        self.output_dim = output_dim # embedding dim\n",
    "        \n",
    "        self.weights = la.Matrix(input_dim,output_dim)\n",
    "        \n",
    "        self.input_indices = list(range(self.input_dim))\n",
    "        self.full_output_indices = list(range(self.output_dim))        \n",
    "        \n",
    "        self.output = la.Vector(output_dim).zero()\n",
    "        \n",
    "        self.input_grad = None;\n",
    "        \n",
    "    def updateOutput(self, input, input_indices):\n",
    "        self.input_indices = input_indices\n",
    "        \n",
    "        self.output.zero()\n",
    "        for index in self.input_indices:\n",
    "            self.output += self.weights[index]\n",
    "    \n",
    "    def updateInputGrad(self, output_grad):\n",
    "        \"do nothing\"\n",
    "    \n",
    "    def accGradParameters(self, input, output_grad, alpha):\n",
    "        # input is all 1s and is not used\n",
    "        for index in self.input_indices:\n",
    "            self.weights[index].addi(output_grad,-alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SparseLinearOutput():\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \n",
    "        self.sparse_output = True\n",
    "        self.sparse_input = False\n",
    "        \n",
    "        self.input_dim = input_dim # embedding dim\n",
    "        self.output_dim = output_dim # output vocab size \n",
    "        \n",
    "        self.weights = la.Matrix(output_dim,input_dim)\n",
    "        \n",
    "        self.output = la.Vector(output_dim).zero()\n",
    "        self.output_indices = list(range(self.output_dim))\n",
    "        self.full_output_indices = list(range(self.output_dim))        \n",
    "        \n",
    "        self.input_grad = la.Vector(self.input_dim).zero()\n",
    "    \n",
    "    def updateOutput(self, input, output_indices=None):\n",
    "        if(output_indices is None):\n",
    "            output_indices = list(range(self.output_dim))\n",
    "        self.output_indices = output_indices\n",
    "        \n",
    "        for index in self.output_indices:\n",
    "            self.output.doti(index,input, self.weights[index])\n",
    "            \n",
    "    def updateInputGrad(self,output_grad):\n",
    "        self.input_grad.set(self.weights[self.output_indices[0]],output_grad[self.output_indices[0]]) \n",
    "        for index in self.output_indices[1:]:\n",
    "            self.input_grad.addi(self.weights[index],output_grad[index]) \n",
    "    \n",
    "    def accGradParameters(self, input, output_grad, alpha):\n",
    "        for index in self.output_indices:\n",
    "            self.weights[index].addi(input,output_grad[index] * -alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSECriterion():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\n",
    "        self.grad = la.Vector(1).zero()\n",
    "        \n",
    "    def forward(self,input,target,output_indices):\n",
    "        self.output = input\n",
    "        self.tmp_error = la.Vector(len(output_indices)).zero()\n",
    "        for i,index in enumerate(output_indices):\n",
    "            self.tmp_error[i] = self.output[index] - target[i]\n",
    "            \n",
    "        self.error = 0\n",
    "        for i in range(len(output_indices)):\n",
    "            self.error += self.tmp_error[i] * self.tmp_error[i]\n",
    "        self.error /= len(self.tmp_error)\n",
    "        \n",
    "        return self.error\n",
    "    \n",
    "    def backward(self,output, target,output_indices):\n",
    "        if(self.grad.size != output.size):\n",
    "            self.grad = la.Vector(output.size).zero()\n",
    "        for i,index in enumerate(output_indices):\n",
    "            self.grad[index] = output[index] - target[i]\n",
    "        return self.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Sequential():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\n",
    "        self.layers = list()\n",
    "\n",
    "    def get(self,i):\n",
    "        return self.layers[i]\n",
    "    \n",
    "    def add(self,layer):\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "        self.output = self.layers[-1].output\n",
    "    \n",
    "    def forward(self,input_indices=(1,3,2), output_indices=(1,2,4)):\n",
    "        \n",
    "        self.layers[0].updateOutput(None, input_indices)\n",
    "        \n",
    "        sparse_output_until_end = False\n",
    "        \n",
    "        \n",
    "        for index in range(len(self.layers)-2):\n",
    "            if(self.layers[index+1].hasSparseOutput()):\n",
    "                sparse_output_until_end = True\n",
    "                \n",
    "            if(sparse_output_until_end):\n",
    "                self.layers[index+1].updateOutput(self.layers[index].getOutput(),output_indices)\n",
    "            else:\n",
    "                self.layers[index+1].updateOutput(self.layers[index].output,self.layers[index+1].getFullOutputIndices())\n",
    "            \n",
    "        self.layers[-1].updateOutput(self.layers[-2].getOutput(),output_indices)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad, output_indices):\n",
    "        \n",
    "        self.layers[-1].updateInputGrad(grad)\n",
    "        sparse_output_until_end = True\n",
    "        for i in reversed(range(len(self.layers)-2)):\n",
    "            if(sparse_output_until_end):\n",
    "                self.layers[i+1].updateInputGrad(self.layers[i+2].input_grad)\n",
    "            else:\n",
    "                self.layers[i+1].updateInputGrad(self.layers[i+2].input_grad)\n",
    "            if(self.layers[i+1].sparse_output == True):\n",
    "                sparse_output_until_end = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Relu(nn.Layer):\n",
    "    def __init__(self, dim):\n",
    "\n",
    "        self.sparse_output = False\n",
    "        self.sparse_input = False\n",
    "    \n",
    "        self.input_dim = dim\n",
    "        self.output_dim = dim\n",
    "        \n",
    "        self.weights = None\n",
    "        self.output_indices = list(range(dim))\n",
    "        \n",
    "        self.output = la.Vector(self.output_dim).zero()\n",
    "        self.input_grad = la.Vector(self.input_dim).zero()\n",
    "        \n",
    "        self.full_output_indices = list(range(self.output_dim))\n",
    "    \n",
    "    def updateOutput(self, input, output_indices = None):\n",
    "\n",
    "        if(output_indices is None):\n",
    "            output_indices = list(range(self.output_dim))\n",
    "        self.output_indices = output_indices\n",
    "        \n",
    "        self.input_grad *= 0\n",
    "        self.input_grad += input\n",
    "        self.input_grad >= 0\n",
    "        \n",
    "        self.output *= 0\n",
    "        self.output += input\n",
    "        self.output *= self.input_grad\n",
    "        \n",
    "    def updateInputGrad(self, output_grad):\n",
    "        self.input_grad *= output_grad\n",
    "        \n",
    "    def accGradParameters(self,input, output_grad, alpha):\n",
    "        \"do nothing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StochasticGradient():\n",
    "    \n",
    "    def __init__(self,mlp,criterion,alpha = 0.0001):\n",
    "        self.mlp = mlp\n",
    "        self.criterion = criterion\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def train(self,input_indices, output_indices, target_values): \n",
    "        \n",
    "        pred = self.mlp.forward(input_indices,output_indices)\n",
    "        error = self.criterion.forward(pred,target_values,output_indices)\n",
    "        self.mlp.backward(self.criterion.backward(pred,target_values,output_indices),output_indices)\n",
    "\n",
    "        # update all but last layer\n",
    "        prev_layer_output = None        \n",
    "        for i in range(len(layers)-1):\n",
    "            mlp.get(i).accGradParameters(prev_layer_output,mlp.get(i+1).getInputGrad(),self.alpha)\n",
    "            prev_layer_output = mlp.get(i).getOutput()\n",
    "                    \n",
    "        # update weights for output with criterion gradient        \n",
    "        mlp.get(len(layers)-1).accGradParameters(mlp.get(len(layers)-2).getOutput(),criterion.grad,self.alpha)\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = la.Seed(1)\n",
    "\n",
    "syn0 = nn.SparseLinearInput(100,64)\n",
    "syn0.weights.uniform(seed)\n",
    "syn0.weights -= 0.5\n",
    "syn0.weights /= 50\n",
    "\n",
    "syn1 = nn.Linear(64,32)\n",
    "syn1.weights.uniform(seed)\n",
    "syn1.weights -= 0.5\n",
    "syn1.weights /= 50\n",
    "\n",
    "syn2 = nn.Linear(32,16)\n",
    "syn2.weights.uniform(seed)\n",
    "syn2.weights -= 0.5\n",
    "syn2.weights /= 50\n",
    "\n",
    "syn3 = nn.SparseLinearOutput(16,100)\n",
    "syn3.weights.uniform(seed)\n",
    "\n",
    "layers = list()\n",
    "layers.append(syn0)\n",
    "layers.append(syn1)\n",
    "layers.append(nn.Relu(32))\n",
    "layers.append(syn2)\n",
    "layers.append(nn.Relu(16))\n",
    "layers.append(syn3)\n",
    "layers.append(nn.Relu(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = nn.Sequential(layers)\n",
    "\n",
    "criterion = nn.MSECriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_indices = (21,22,23,24,26,27,28)\n",
    "output_indices = (11,12,13,14,15,16,10,14)\n",
    "\n",
    "target = la.Vector(len(output_indices))\n",
    "target.zero()\n",
    "target.set(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optim = nn.StochasticGradient(mlp,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 7.87 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "optim.train(input_indices,output_indices,target,alpha=0.001,iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12497355043888092\n",
      "0.1249733716249466\n",
      "0.12497317790985107\n",
      "0.12497298419475555\n",
      "0.12497280538082123\n",
      "0.12497261166572571\n",
      "0.12497241795063019\n",
      "0.12497222423553467\n",
      "0.12497203052043915\n",
      "0.12497183680534363\n",
      "0.12497164309024811\n",
      "0.1249714344739914\n",
      "0.12497124075889587\n",
      "0.12497104704380035\n",
      "0.12497083842754364\n",
      "0.12497064471244812\n",
      "0.1249704360961914\n",
      "0.12497024238109589\n",
      "0.12497003376483917\n",
      "0.12496982514858246\n",
      "0.12496961653232574\n",
      "0.12496940791606903\n",
      "0.12496919929981232\n",
      "0.1249689906835556\n",
      "0.12496878206729889\n",
      "0.12496855854988098\n",
      "0.12496834993362427\n",
      "0.12496812641620636\n",
      "0.12496791779994965\n",
      "0.12496769428253174\n",
      "0.12496748566627502\n",
      "0.12496726214885712\n",
      "0.12496703863143921\n",
      "0.1249668151140213\n",
      "0.1249665915966034\n",
      "0.12496637552976608\n",
      "0.12496613711118698\n",
      "0.12496591359376907\n",
      "0.12496569007635117\n",
      "0.12496545165777206\n",
      "0.12496522814035416\n",
      "0.12496498972177505\n",
      "0.12496475130319595\n",
      "0.12496451288461685\n",
      "0.12496427446603775\n",
      "0.12496403604745865\n",
      "0.12496379762887955\n",
      "0.12496356666088104\n",
      "0.12496331334114075\n",
      "0.12496307492256165\n",
      "0.12496282160282135\n",
      "0.12496258318424225\n",
      "0.12496232986450195\n",
      "0.12496207654476166\n",
      "0.12496183067560196\n",
      "0.12496157735586166\n",
      "0.12496132403612137\n",
      "0.12496107071638107\n",
      "0.12496080249547958\n",
      "0.12496054917573929\n",
      "0.1249602809548378\n",
      "0.1249600276350975\n",
      "0.12495975941419601\n",
      "0.12495949119329453\n",
      "0.12495922297239304\n",
      "0.12495895475149155\n",
      "0.12495868653059006\n",
      "0.12495840340852737\n",
      "0.12495813518762589\n",
      "0.1249578520655632\n",
      "0.12495756894350052\n",
      "0.12495730072259903\n",
      "0.12495701760053635\n",
      "0.12495674192905426\n",
      "0.12495644390583038\n",
      "0.1249561607837677\n",
      "0.12495587766170502\n",
      "0.12495557963848114\n",
      "0.12495529651641846\n",
      "0.12495499849319458\n",
      "0.1249547004699707\n",
      "0.12495440244674683\n",
      "0.12495410442352295\n",
      "0.12495379149913788\n",
      "0.124953493475914\n",
      "0.12495318055152893\n",
      "0.12495288252830505\n",
      "0.12495256960391998\n",
      "0.12495225667953491\n",
      "0.12495194375514984\n",
      "0.12495163083076477\n",
      "0.1249513030052185\n",
      "0.12495099008083344\n",
      "0.12495066225528717\n",
      "0.1249503344297409\n",
      "0.12495002150535583\n",
      "0.12494969367980957\n",
      "0.12494935095310211\n",
      "0.12494902312755585\n",
      "0.12494868040084839\n",
      "0.12494835257530212\n",
      "0.12494800984859467\n",
      "0.12494766712188721\n",
      "0.12494732439517975\n",
      "0.12494698166847229\n",
      "0.12494662404060364\n",
      "0.12494628131389618\n",
      "0.12494592368602753\n",
      "0.12494556605815887\n",
      "0.12494520843029022\n",
      "0.12494485080242157\n",
      "0.12494449317455292\n",
      "0.12494412064552307\n",
      "0.12494376301765442\n",
      "0.12494339048862457\n",
      "0.12494301795959473\n",
      "0.12494264543056488\n",
      "0.12494225800037384\n",
      "0.124941885471344\n",
      "0.12494149804115295\n",
      "0.12494111061096191\n",
      "0.12494072318077087\n",
      "0.12494034320116043\n",
      "0.12493995577096939\n",
      "0.12493955343961716\n",
      "0.12493915110826492\n",
      "0.12493874877691269\n",
      "0.12493834644556046\n",
      "0.12493794411420822\n",
      "0.12493755668401718\n",
      "0.12493714690208435\n",
      "0.12493672966957092\n",
      "0.1249363124370575\n",
      "0.12493589520454407\n",
      "0.12493546307086945\n",
      "0.12493504583835602\n",
      "0.1249346137046814\n",
      "0.12493418157100677\n",
      "0.12493373453617096\n",
      "0.12493330240249634\n",
      "0.12493286281824112\n",
      "0.1249324232339859\n",
      "0.12493198364973068\n",
      "0.12493153661489487\n",
      "0.12493108958005905\n",
      "0.12493062764406204\n",
      "0.12493016570806503\n",
      "0.12492970377206802\n",
      "0.12492922693490982\n",
      "0.12492876499891281\n",
      "0.12492828816175461\n",
      "0.1249278113245964\n",
      "0.1249273344874382\n",
      "0.1249268427491188\n",
      "0.1249263659119606\n",
      "0.1249258741736412\n",
      "0.12492536753416061\n",
      "0.12492488324642181\n",
      "0.12492437660694122\n",
      "0.12492386996746063\n",
      "0.12492336332798004\n",
      "0.12492285668849945\n",
      "0.12492234259843826\n",
      "0.12492182105779648\n",
      "0.1249212995171547\n",
      "0.12492077797651291\n",
      "0.12492024153470993\n",
      "0.12491970509290695\n",
      "0.12491916865110397\n",
      "0.1249186173081398\n",
      "0.12491808086633682\n",
      "0.12491752952337265\n",
      "0.12491696327924728\n",
      "0.12491641193628311\n",
      "0.12491584569215775\n",
      "0.12491527944803238\n",
      "0.12491469830274582\n",
      "0.12491413205862045\n",
      "0.12491355091333389\n",
      "0.12491295486688614\n",
      "0.12491237372159958\n",
      "0.12491179257631302\n",
      "0.12491119652986526\n",
      "0.12491058558225632\n",
      "0.12490998953580856\n",
      "0.12490937858819962\n",
      "0.12490875273942947\n",
      "0.12490814179182053\n",
      "0.12490752339363098\n",
      "0.12490688264369965\n",
      "0.1249062567949295\n",
      "0.12490561604499817\n",
      "0.12490497529506683\n",
      "0.1249043196439743\n",
      "0.12490366399288177\n",
      "0.12490301579236984\n",
      "0.12490234524011612\n",
      "0.1249016746878624\n",
      "0.12490100413560867\n",
      "0.12490033358335495\n",
      "0.12489964812994003\n",
      "0.12489894777536392\n",
      "0.124898262321949\n",
      "0.1248975470662117\n",
      "0.12489684671163559\n",
      "0.12489613145589828\n",
      "0.12489541620016098\n",
      "0.12489472329616547\n",
      "0.12489399313926697\n",
      "0.12489324808120728\n",
      "0.12489251792430878\n",
      "0.12489177286624908\n",
      "0.1248910129070282\n",
      "0.1248902678489685\n",
      "0.12488950788974762\n",
      "0.12488873302936554\n",
      "0.12488795816898346\n",
      "0.12488719075918198\n",
      "0.1248864009976387\n",
      "0.12488560378551483\n",
      "0.12488481402397156\n",
      "0.12488400191068649\n",
      "0.12488319724798203\n",
      "0.12488237768411636\n",
      "0.12488154321908951\n",
      "0.12488070875406265\n",
      "0.12487989664077759\n",
      "0.12487904727458954\n",
      "0.1248781830072403\n",
      "0.12487733364105225\n",
      "0.12487645447254181\n",
      "0.12487559020519257\n",
      "0.12487469613552094\n",
      "0.1248738169670105\n",
      "0.12487291544675827\n",
      "0.12487202137708664\n",
      "0.12487111240625381\n",
      "0.1248701959848404\n",
      "0.12486927211284637\n",
      "0.12486833333969116\n",
      "0.12486739456653595\n",
      "0.12486645579338074\n",
      "0.12486550211906433\n",
      "0.12486453354358673\n",
      "0.12486355751752853\n",
      "0.12486257404088974\n",
      "0.12486159056425095\n",
      "0.12486059218645096\n",
      "0.12485957890748978\n",
      "0.1248585656285286\n",
      "0.12485755234956741\n",
      "0.12485653162002563\n",
      "0.12485548853874207\n",
      "0.12485446780920029\n",
      "0.12485340237617493\n",
      "0.12485234439373016\n",
      "0.12485125660896301\n",
      "0.12485016882419586\n",
      "0.12484908103942871\n",
      "0.12484797835350037\n",
      "0.12484686821699142\n",
      "0.12484573572874069\n",
      "0.12484461069107056\n",
      "0.12484348565340042\n",
      "0.1248423382639885\n",
      "0.12484117597341537\n",
      "0.12483999878168106\n",
      "0.12483881413936615\n",
      "0.12483762204647064\n",
      "0.12483641505241394\n",
      "0.12483520805835724\n",
      "0.12483397871255875\n",
      "0.12483275681734085\n",
      "0.12483152747154236\n",
      "0.12483027577400208\n",
      "0.1248289942741394\n",
      "0.12482771277427673\n",
      "0.12482643127441406\n",
      "0.1248251274228096\n",
      "0.12482383102178574\n",
      "0.12482249736785889\n",
      "0.12482116371393204\n",
      "0.12481982260942459\n",
      "0.12481845170259476\n",
      "0.12481708824634552\n",
      "0.1248156875371933\n",
      "0.12481430172920227\n",
      "0.12481289356946945\n",
      "0.12481147050857544\n",
      "0.12481004744768143\n",
      "0.12480860203504562\n",
      "0.12480712682008743\n",
      "0.12480565905570984\n",
      "0.12480418384075165\n",
      "0.12480267137289047\n",
      "0.1248011589050293\n",
      "0.12479962408542633\n",
      "0.12479807436466217\n",
      "0.12479653209447861\n",
      "0.12479495257139206\n",
      "0.12479336559772491\n",
      "0.12479176372289658\n",
      "0.12479013949632645\n",
      "0.12478853762149811\n",
      "0.12478688359260559\n",
      "0.12478520721197128\n",
      "0.12478353083133698\n",
      "0.12478183209896088\n",
      "0.12478012591600418\n",
      "0.12477840483188629\n",
      "0.12477664649486542\n",
      "0.12477489560842514\n",
      "0.12477312237024307\n",
      "0.12477132678031921\n",
      "0.12476952373981476\n",
      "0.12476769089698792\n",
      "0.12476585805416107\n",
      "0.12476398795843124\n",
      "0.12476210296154022\n",
      "0.124760203063488\n",
      "0.1247582957148552\n",
      "0.1247563287615776\n",
      "0.12475436180830002\n",
      "0.12475238740444183\n",
      "0.12475038319826126\n",
      "0.1247483491897583\n",
      "0.12474629282951355\n",
      "0.1247442364692688\n",
      "0.12474215775728226\n",
      "0.12474003434181213\n",
      "0.12473791837692261\n",
      "0.1247357726097107\n",
      "0.12473360449075699\n",
      "0.12473141402006149\n",
      "0.12472919374704361\n",
      "0.12472694367170334\n",
      "0.12472469359636307\n",
      "0.1247224286198616\n",
      "0.12472011893987656\n",
      "0.12471779435873032\n",
      "0.12471543252468109\n",
      "0.12471304833889008\n",
      "0.12471064925193787\n",
      "0.12470822781324387\n",
      "0.12470578402280807\n",
      "0.1247033029794693\n",
      "0.12470080703496933\n",
      "0.12469827383756638\n",
      "0.12469571083784103\n",
      "0.1246931403875351\n",
      "0.12469056248664856\n",
      "0.12468791007995605\n",
      "0.12468525022268295\n",
      "0.12468253821134567\n",
      "0.1246798112988472\n",
      "0.12467704713344574\n",
      "0.12467428296804428\n",
      "0.12467144429683685\n",
      "0.12466862052679062\n",
      "0.12466572225093842\n",
      "0.12466282397508621\n",
      "0.12465987354516983\n",
      "0.12465691566467285\n",
      "0.12465391308069229\n",
      "0.12465085834264755\n",
      "0.12464779615402222\n",
      "0.1246447041630745\n",
      "0.12464156746864319\n",
      "0.1246384009718895\n",
      "0.12463519722223282\n",
      "0.12463195621967316\n",
      "0.12462867796421051\n",
      "0.12462536990642548\n",
      "0.12462202459573746\n",
      "0.12461863458156586\n",
      "0.12461521476507187\n",
      "0.12461178004741669\n",
      "0.12460827082395554\n",
      "0.1246047094464302\n",
      "0.12460115551948547\n",
      "0.12459752708673477\n",
      "0.12459386140108109\n",
      "0.1245901957154274\n",
      "0.12458643317222595\n",
      "0.12458263337612152\n",
      "0.1245788186788559\n",
      "0.12457495182752609\n",
      "0.1245710477232933\n",
      "0.12456707656383514\n",
      "0.12456308305263519\n",
      "0.12455901503562927\n",
      "0.12455491721630096\n",
      "0.12455078214406967\n",
      "0.124546580016613\n",
      "0.12454234808683395\n",
      "0.12453804910182953\n",
      "0.12453371286392212\n",
      "0.12452929466962814\n",
      "0.12452486157417297\n",
      "0.12452037632465363\n",
      "0.12451580911874771\n",
      "0.12451118975877762\n",
      "0.12450654804706573\n",
      "0.12450182437896729\n",
      "0.12449705600738525\n",
      "0.12449222058057785\n",
      "0.12448732554912567\n",
      "0.12448237836360931\n",
      "0.12447737902402878\n",
      "0.12447229772806168\n",
      "0.12446717172861099\n",
      "0.12446197122335434\n",
      "0.12445671856403351\n",
      "0.12445139139890671\n",
      "0.12444601207971573\n",
      "0.12444055080413818\n",
      "0.12443502247333527\n",
      "0.12442941963672638\n",
      "0.12442377209663391\n",
      "0.12441803514957428\n",
      "0.12441223114728928\n",
      "0.12440633028745651\n",
      "0.12440036237239838\n",
      "0.12439431995153427\n",
      "0.124388188123703\n",
      "0.12438197433948517\n",
      "0.12437567859888077\n",
      "0.124369315803051\n",
      "0.12436287850141525\n",
      "0.12435633689165115\n",
      "0.12434972822666168\n",
      "0.12434300780296326\n",
      "0.12433622777462006\n",
      "0.1243293508887291\n",
      "0.12432238459587097\n",
      "0.12431532889604568\n",
      "0.12430816888809204\n",
      "0.12430093437433243\n",
      "0.12429357320070267\n",
      "0.12428615987300873\n",
      "0.12427859753370285\n",
      "0.12427094578742981\n",
      "0.1242632195353508\n",
      "0.12425536662340164\n",
      "0.12424742430448532\n",
      "0.12423934787511826\n",
      "0.12423118948936462\n",
      "0.12422290444374084\n",
      "0.12421451508998871\n",
      "0.12420601397752762\n",
      "0.12419737875461578\n",
      "0.124188631772995\n",
      "0.12417978048324585\n",
      "0.12417078763246536\n",
      "0.12416167557239532\n",
      "0.12415242195129395\n",
      "0.12414303421974182\n",
      "0.12413355708122253\n",
      "0.1241239383816719\n",
      "0.12411414086818695\n",
      "0.12410423159599304\n",
      "0.12409418076276779\n",
      "0.1240839883685112\n",
      "0.12407366186380386\n",
      "0.1240631714463234\n",
      "0.12405253946781158\n",
      "0.12404173612594604\n",
      "0.12403079867362976\n",
      "0.12401966750621796\n",
      "0.1240084171295166\n",
      "0.12399698793888092\n",
      "0.1239853948354721\n",
      "0.12397361546754837\n",
      "0.1239616647362709\n",
      "0.12394954264163971\n",
      "0.12393724173307419\n",
      "0.12392475455999374\n",
      "0.12391209602355957\n",
      "0.12389922142028809\n",
      "0.12388617545366287\n",
      "0.12387292832136154\n",
      "0.1238594800233841\n",
      "0.12384581565856934\n",
      "0.12383195757865906\n",
      "0.12381788343191147\n",
      "0.12380358576774597\n",
      "0.12378907203674316\n",
      "0.12377434968948364\n",
      "0.12375936657190323\n",
      "0.1237441748380661\n",
      "0.12372875958681107\n",
      "0.12371309101581573\n",
      "0.12369716912508011\n",
      "0.12368099391460419\n",
      "0.12366458773612976\n",
      "0.12364789843559265\n",
      "0.12363094091415405\n",
      "0.12361373752355576\n",
      "0.12359624356031418\n",
      "0.12357846647500992\n",
      "0.12356042116880417\n",
      "0.12354207783937454\n",
      "0.12352342158555984\n",
      "0.12350450456142426\n",
      "0.12348524481058121\n",
      "0.1234656572341919\n",
      "0.1234457939863205\n",
      "0.12342557311058044\n",
      "0.12340500205755234\n",
      "0.12338414788246155\n",
      "0.12336290627717972\n",
      "0.12334131449460983\n",
      "0.12331938743591309\n",
      "0.1232970729470253\n",
      "0.12327439337968826\n",
      "0.12325131893157959\n",
      "0.12322785705327988\n",
      "0.12320400029420853\n",
      "0.12317976355552673\n",
      "0.12315510958433151\n",
      "0.12313000857830048\n",
      "0.12310449779033661\n",
      "0.12307852506637573\n",
      "0.12305212765932083\n",
      "0.12302527576684952\n",
      "0.1229979619383812\n",
      "0.12297014892101288\n",
      "0.12294186651706696\n",
      "0.12291308492422104\n",
      "0.12288382649421692\n",
      "0.12285400927066803\n",
      "0.12282370030879974\n",
      "0.12279284000396729\n",
      "0.12276144325733185\n",
      "0.12272946536540985\n",
      "0.12269695848226547\n",
      "0.12266384065151215\n",
      "0.12263012677431107\n",
      "0.12259585410356522\n",
      "0.12256091833114624\n",
      "0.12252536416053772\n",
      "0.12248916178941727\n",
      "0.12245231121778488\n",
      "0.12241479009389877\n",
      "0.12237658351659775\n",
      "0.12233768403530121\n",
      "0.12229805439710617\n",
      "0.12225772440433502\n",
      "0.12221665680408478\n",
      "0.12217480689287186\n",
      "0.12213219702243805\n",
      "0.12208880484104156\n",
      "0.12204461544752121\n",
      "0.1219995766878128\n",
      "0.12195368856191635\n",
      "0.12190694361925125\n",
      "0.12185931950807571\n",
      "0.12181080132722855\n",
      "0.12176138162612915\n",
      "0.12171100825071335\n",
      "0.12165970355272293\n",
      "0.12160743772983551\n",
      "0.1215541809797287\n",
      "0.1214999258518219\n",
      "0.12144464254379272\n",
      "0.12138830870389938\n",
      "0.12133089452981949\n",
      "0.12127240002155304\n",
      "0.12121278047561646\n",
      "0.12115204334259033\n",
      "0.1210901290178299\n",
      "0.12102705985307693\n",
      "0.12096275389194489\n",
      "0.12089724838733673\n",
      "0.1208304837346077\n",
      "0.12076243758201599\n",
      "0.12069310992956161\n",
      "0.12062244117259979\n",
      "0.12055042386054993\n",
      "0.12047702819108963\n",
      "0.12040222436189651\n",
      "0.12032601237297058\n",
      "0.12024834007024765\n",
      "0.12016917020082474\n",
      "0.12008850276470184\n",
      "0.12000630050897598\n",
      "0.11992253363132477\n",
      "0.11983717232942581\n",
      "0.11975019425153732\n",
      "0.11966156959533691\n",
      "0.1195712611079216\n",
      "0.119479238986969\n",
      "0.11938551068305969\n",
      "0.11929000914096832\n",
      "0.11919272691011429\n",
      "0.1190936267375946\n",
      "0.11899267137050629\n",
      "0.11888984590768814\n",
      "0.11878512799739838\n",
      "0.11867845803499222\n",
      "0.11856984347105026\n",
      "0.11845925450325012\n",
      "0.11834666132926941\n",
      "0.11823201924562454\n",
      "0.11811532080173492\n",
      "0.11799651384353638\n",
      "0.1178756132721901\n",
      "0.11775258183479309\n",
      "0.11762738972902298\n",
      "0.11750002205371857\n",
      "0.11737043410539627\n",
      "0.11723865568637848\n",
      "0.11710459738969803\n",
      "0.1169683113694191\n",
      "0.11682972311973572\n",
      "0.11668886244297028\n",
      "0.11654569953680038\n",
      "0.11640020459890366\n",
      "0.11625239253044128\n",
      "0.11610224097967148\n",
      "0.11594973504543304\n",
      "0.11579488962888718\n",
      "0.11563768237829208\n",
      "0.11547813564538956\n",
      "0.1153162345290184\n",
      "0.11515197902917862\n",
      "0.1149853765964508\n",
      "0.11481644213199615\n",
      "0.11464520543813705\n",
      "0.11447165161371231\n",
      "0.11429580301046371\n",
      "0.11411767452955246\n",
      "0.11393729597330093\n",
      "0.11375470459461212\n",
      "0.11356993019580841\n",
      "0.1133829727768898\n",
      "0.11319388449192047\n",
      "0.11300268769264221\n",
      "0.11280947923660278\n",
      "0.1126142144203186\n",
      "0.11241701245307922\n",
      "0.11221791058778763\n",
      "0.11201690882444382\n",
      "0.11181413382291794\n",
      "0.11160960048437119\n",
      "0.11140336841344833\n",
      "0.11119551211595535\n",
      "0.11098609119653702\n",
      "0.11077519506216049\n",
      "0.11056285351514816\n",
      "0.11034917831420898\n",
      "0.11013420671224594\n",
      "0.109918013215065\n",
      "0.10970069468021393\n",
      "0.10948231816291809\n",
      "0.10926295071840286\n",
      "0.109042689204216\n",
      "0.1088215783238411\n",
      "0.1085997223854065\n",
      "0.10837716609239578\n",
      "0.10815399885177612\n",
      "0.10793029516935349\n",
      "0.10770611464977264\n",
      "0.10748150199651718\n",
      "0.10725659132003784\n",
      "0.10703138262033463\n",
      "0.1068059504032135\n",
      "0.10658033192157745\n",
      "0.10635460913181305\n",
      "0.10612883418798447\n",
      "0.10590298473834991\n",
      "0.10567715018987656\n",
      "0.10545134544372559\n",
      "0.1052255928516388\n",
      "0.10499992966651917\n",
      "0.1047743409872055\n",
      "0.1045488715171814\n",
      "0.10432347655296326\n",
      "0.10409821569919586\n",
      "0.10387303680181503\n",
      "0.10364791005849838\n",
      "0.1034228652715683\n",
      "0.10319784283638\n",
      "0.10297279059886932\n",
      "0.10274769365787506\n",
      "0.10252252221107483\n",
      "0.10229720920324326\n",
      "0.10207170248031616\n",
      "0.10184594243764877\n",
      "0.10161985456943512\n",
      "0.10139336436986923\n",
      "0.10116644203662872\n",
      "0.10093896836042404\n",
      "0.10071083903312683\n",
      "0.1004820317029953\n",
      "0.10025240480899811\n",
      "0.10002188384532928\n",
      "0.09979037940502167\n",
      "0.0995577722787857\n",
      "0.09932400286197662\n",
      "0.09908890724182129\n",
      "0.09885243326425552\n",
      "0.09861444681882858\n",
      "0.09837482869625092\n",
      "0.09813349694013596\n",
      "0.09789033979177475\n",
      "0.09764522314071655\n",
      "0.09739804267883301\n",
      "0.09714869409799576\n",
      "0.09689704328775406\n",
      "0.09664297848939896\n",
      "0.09638640284538269\n",
      "0.09612717479467392\n",
      "0.09586519747972488\n",
      "0.09560032933950424\n",
      "0.09533248841762543\n",
      "0.09506151080131531\n",
      "0.09478730708360672\n",
      "0.09450976550579071\n",
      "0.09422876685857773\n",
      "0.09394416958093643\n",
      "0.09365586936473846\n",
      "0.09336373209953308\n",
      "0.09306765347719193\n",
      "0.09276752173900604\n",
      "0.09246319532394409\n",
      "0.09216903895139694\n",
      "0.0918726697564125\n",
      "0.09157276898622513\n",
      "0.09126920998096466\n",
      "0.09096185117959976\n",
      "0.09065064787864685\n",
      "0.0903354212641716\n",
      "0.09001783281564713\n",
      "0.08972924202680588\n",
      "0.08943842351436615\n",
      "0.0891452431678772\n",
      "0.08884956687688828\n",
      "0.08855129033327103\n",
      "0.08825025707483292\n",
      "0.08794637769460678\n",
      "0.08763948082923889\n",
      "0.08732952922582626\n",
      "0.08701632171869278\n",
      "0.08669978380203247\n",
      "0.08637978136539459\n",
      "0.08605624735355377\n",
      "0.08572903275489807\n",
      "0.08539804071187973\n",
      "0.0850631445646286\n",
      "0.08472426235675812\n",
      "0.08438128978013992\n",
      "0.08403412997722626\n",
      "0.08368264138698578\n",
      "0.0833267942070961\n",
      "0.08296642452478409\n",
      "0.08260150998830795\n",
      "0.08223187178373337\n",
      "0.08185749500989914\n",
      "0.08147823810577393\n",
      "0.08109407126903534\n",
      "0.08070483803749084\n",
      "0.08037339895963669\n",
      "0.08005567640066147\n",
      "0.07975272089242935\n",
      "0.07945006340742111\n",
      "0.07914751023054123\n",
      "0.07884491235017776\n",
      "0.07854204624891281\n",
      "0.07823873311281204\n",
      "0.0779348611831665\n",
      "0.07763021439313889\n",
      "0.07732462882995605\n",
      "0.07701800018548965\n",
      "0.07671013474464417\n",
      "0.07640094310045242\n",
      "0.0760902538895607\n",
      "0.07577796280384064\n",
      "0.07546395808458328\n",
      "0.07517284899950027\n",
      "0.07490979135036469\n",
      "0.07464814186096191\n",
      "0.07438773661851883\n",
      "0.0741283968091011\n",
      "0.07386993616819382\n",
      "0.07361222058534622\n",
      "0.07335507869720459\n",
      "0.07309833914041519\n",
      "0.07284187525510788\n",
      "0.0725855827331543\n",
      "0.07232929766178131\n",
      "0.07207293063402176\n",
      "0.0718163549900055\n",
      "0.07155948132276535\n",
      "0.07130218297243118\n",
      "0.07104441523551941\n",
      "0.07078605890274048\n",
      "0.07052703201770782\n",
      "0.07026734203100204\n",
      "0.07000680267810822\n",
      "0.06974536925554276\n",
      "0.06948301196098328\n",
      "0.0692196860909462\n",
      "0.06895530223846436\n",
      "0.06868983060121536\n",
      "0.06842324137687683\n",
      "0.0681554526090622\n",
      "0.06788644939661026\n",
      "0.06761620193719864\n",
      "0.06734465062618256\n",
      "0.06707179546356201\n",
      "0.06679755449295044\n",
      "0.06652196496725082\n",
      "0.0662449300289154\n",
      "0.06596648693084717\n",
      "0.06568656861782074\n",
      "0.06540518254041672\n",
      "0.06512228399515152\n",
      "0.06483787298202515\n",
      "0.06455191224813461\n",
      "0.06426438689231873\n",
      "0.06397529691457748\n",
      "0.0636846125125885\n",
      "0.06339232623577118\n",
      "0.06309841573238373\n",
      "0.06280286610126495\n",
      "0.06250568479299545\n",
      "0.0622578002512455\n",
      "0.06205017492175102\n",
      "0.06184455379843712\n",
      "0.061640769243240356\n",
      "0.0614386647939682\n",
      "0.06123810261487961\n",
      "0.06103892624378204\n",
      "0.06084102392196655\n",
      "0.060644276440143585\n",
      "0.060448579490184784\n",
      "0.06025383248925209\n",
      "0.06005994975566864\n",
      "0.05986684933304787\n",
      "0.0596744604408741\n",
      "0.059482719749212265\n",
      "0.059291549026966095\n",
      "0.0591009184718132\n",
      "0.05891075357794762\n",
      "0.05872102081775665\n",
      "0.05853167921304703\n",
      "0.058342695236206055\n",
      "0.05815402790904045\n",
      "0.05796565115451813\n",
      "0.05777754262089729\n",
      "0.05758966878056526\n",
      "0.05740202218294144\n",
      "0.057214561849832535\n",
      "0.05702729523181915\n",
      "0.0568401925265789\n",
      "0.056653257459402084\n",
      "0.056466467678546906\n",
      "0.05627981200814247\n",
      "0.056093279272317886\n",
      "0.055906884372234344\n",
      "0.05572059005498886\n",
      "0.05553441122174263\n",
      "0.055348340421915054\n",
      "0.05516237020492554\n",
      "0.05497649684548378\n",
      "0.05479073524475098\n",
      "0.054605063050985336\n",
      "0.054419491440057755\n",
      "0.054234012961387634\n",
      "0.054048627614974976\n",
      "0.053863346576690674\n",
      "0.05367816239595413\n",
      "0.05349308252334595\n",
      "0.05330810323357582\n",
      "0.05312322452664375\n",
      "0.052938446402549744\n",
      "0.0527537576854229\n",
      "0.0525691956281662\n",
      "0.05238475278019905\n",
      "0.05220040678977966\n",
      "0.052016183733940125\n",
      "0.05183207243680954\n",
      "0.05164809897542\n",
      "0.05146423354744911\n",
      "0.05128050968050957\n",
      "0.05109691247344017\n",
      "0.05091343820095062\n",
      "0.05073012039065361\n",
      "0.050546932965517044\n",
      "0.05036388337612152\n",
      "0.05018099024891853\n",
      "0.049998246133327484\n",
      "0.04981565475463867\n",
      "0.04963322728872299\n",
      "0.04945095255970955\n",
      "0.04926884546875954\n",
      "0.049086906015872955\n",
      "0.0489051416516304\n",
      "0.04872354492545128\n",
      "0.048542141914367676\n",
      "0.0483609102666378\n",
      "0.048179857432842255\n",
      "0.04799901694059372\n",
      "0.04781835153698921\n",
      "0.04763788729906082\n",
      "0.04745762422680855\n",
      "0.04727756232023239\n",
      "0.04709769785404205\n",
      "0.04691806435585022\n",
      "0.0467386320233345\n",
      "0.046559423208236694\n",
      "0.0463804267346859\n",
      "0.04620165377855301\n",
      "0.04602311924099922\n",
      "0.04584481194615364\n",
      "0.04566673934459686\n",
      "0.04548889398574829\n",
      "0.04531130939722061\n",
      "0.04513395577669144\n",
      "0.044956862926483154\n",
      "0.04478001222014427\n",
      "0.044603414833545685\n",
      "0.04442707821726799\n",
      "0.04425100237131119\n",
      "0.04407519847154617\n",
      "0.04389966279268265\n",
      "0.04372439160943031\n",
      "0.04354941099882126\n",
      "0.043374691158533096\n",
      "0.043200261890888214\n",
      "0.043026115745306015\n",
      "0.0428522527217865\n",
      "0.042678676545619965\n",
      "0.042505402117967606\n",
      "0.04233242943882942\n",
      "0.04215974733233452\n",
      "0.04198736697435379\n",
      "0.04181529954075813\n",
      "0.04164354130625725\n",
      "0.04147208854556084\n",
      "0.04130095615983009\n",
      "0.04113014042377472\n",
      "0.04095964878797531\n",
      "0.04078948125243187\n",
      "0.040619637817144394\n",
      "0.040450118482112885\n",
      "0.040280938148498535\n",
      "0.04011208936572075\n",
      "0.03994358703494072\n",
      "0.039775412529706955\n",
      "0.039607591927051544\n",
      "0.039440106600522995\n",
      "0.0392729751765728\n",
      "0.03910620138049126\n",
      "0.03893978148698807\n",
      "0.038773708045482635\n",
      "0.03860799968242645\n",
      "0.03844265267252922\n",
      "0.038277674466371536\n",
      "0.0381130650639534\n",
      "0.037948813289403915\n",
      "0.03778493404388428\n",
      "0.03762143477797508\n",
      "0.03745831176638603\n",
      "0.037295568734407425\n",
      "0.03713320568203926\n",
      "0.03697121888399124\n",
      "0.03680961951613426\n",
      "0.03664841502904892\n",
      "0.03648759424686432\n",
      "0.03632717207074165\n",
      "0.03616712987422943\n",
      "0.03600750118494034\n",
      "0.035848259925842285\n",
      "0.03568943217396736\n",
      "0.03553099185228348\n",
      "0.03537296503782272\n",
      "0.0352153405547142\n",
      "0.03505812585353851\n",
      "0.034901320934295654\n",
      "0.03474493324756622\n",
      "0.03458895534276962\n",
      "0.03443339467048645\n",
      "0.034278251230716705\n",
      "0.03412352129817009\n",
      "0.033969223499298096\n",
      "0.03381533920764923\n",
      "0.03366188332438469\n",
      "0.03350885957479477\n",
      "0.033356256783008575\n",
      "0.033204082399606705\n",
      "0.033052340149879456\n",
      "0.032901033759117126\n",
      "0.03275015950202942\n",
      "0.03259972110390663\n",
      "0.032449714839458466\n",
      "0.032300155609846115\n",
      "0.032151032239198685\n",
      "0.03200234845280647\n"
     ]
    }
   ],
   "source": [
    "for iter in range(1000):\n",
    "    print(optim.train(input_indices,output_indices,target,alpha=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iter in range(1000):\n",
    "    print(optim.train(input_indices,output_indices,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_sample = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<linguamind.linalg.Tensor; proxy of <Swig Object of type 'Tensor *' at 0x1078b4e70> >"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn0 = nn.SparseLinearInput(1000,50)\n",
    "syn0.weights.uniform()\n",
    "syn0.weights -= 0.5\n",
    "syn0.weights /= 50\n",
    "\n",
    "syn1 = nn.SparseLinearOutput(50,1000,n_sample)\n",
    "syn1.weights.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = nn.Sequential()\n",
    "mlp.add(syn0)\n",
    "mlp.add(syn1)\n",
    "\n",
    "criterion = nn.MSECriterion(1,n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = la.Tensor((1,10))\n",
    "target.zero()\n",
    "target.set(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_indices = (1,2,4,5)\n",
    "pred = 3\n",
    "neg_samples = list([6,7,8,9,10,11,12,13,14])\n",
    "target_indices = list([pred]) + neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion.forwards(mlp.forward(context_indices,target_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<linguamind.linalg.Tensor; proxy of <Swig Object of type 'Tensor *' at 0x1078b4630> >"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion.backwards(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = la.Tensor((1,50)).uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syn1.updateOutput(input,(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syn1.output.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = nlp.Text(\"hello world\",\"BIIIIOBIIII\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = text.getVocab(\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.getSequence('tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# text = lm.Text(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = lm.Text(\"hello world\",\"BIIIIOBIIII\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = text.getVocab(\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(vocab.size):\n",
    "    print vocab.getTermAtIndex(i).letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.getSequence(\"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NLP Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Types of Component:\n",
    "- word level\n",
    "- sentence level\n",
    "- document level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = lm.Text(\"/my/folder/of/files\") # atempts to autodetect file type\n",
    "c = lm.Text(path=\"myfile.txt\",type=\"raw\")\n",
    "c = lm.Text(path=\"myfile.txt\",type=\"csv-token\") # one token per line in CSV\n",
    "c = lm.Text(path=\"myfile.txt\",type=\"csv-segment\") # text segment per line in CSV\n",
    "c = lm.Text(path=\"myfile.txt\",type=\"prefix__label__\") # one sentence per line with special labels\n",
    "c = lm.Text(path=\"myfile.txt\",type=\"json\")\n",
    "\n",
    "# document level raw text\n",
    "d1 = lm.Text(\"This is my sentence.\")\n",
    "d2 = lm.Text(\"myfile.txt\")\n",
    "\n",
    "c = lm.Text([d1,d2]) #form documents into Corpus\n",
    "\n",
    "# document level text with labels\n",
    "contents = {}\n",
    "contents[\"text\"] = \"This is my sentence. This is another one.\"\n",
    "contents[\"date\"] = 1997\n",
    "d = lm.Text(contents)\n",
    "\n",
    "# sentence level text with labels\n",
    "contents = {}\n",
    "contents[\"text\"] = \"This is my sentence.\"\n",
    "contents[\"sentiment\"] = \"Positive\"\n",
    "contents[\"speaker\"] = \"John Locke\"\n",
    "d = lm.Text(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok = lm.Segmenter(name=\"tokenizer\",lang=\"en\")\n",
    "eos = lm.Segmenter(name=\"eos\",lang=\"en\")\n",
    "\n",
    "# document level text with labels\n",
    "contents = {}\n",
    "contents[\"text\"] = \"This is my sentence. This is another one.\"\n",
    "contents[\"date\"] = 1997\n",
    "d = lm.Text(contents)\n",
    "\n",
    "tok.segment(d)\n",
    "eos.segment(d)\n",
    "\n",
    "print d[\"text\"] # \"This is my sentence. This is another one.\"\n",
    "print d[\"tokens\"] # [\"This\", \"is\"...\n",
    "print d[\"sentences\"] # [[\"This\", \"is\", \"my\", \"sentence\",\".\"],[\"This\",\"is\",\"another\",\"one\",\".\"]]                \n",
    "                     \n",
    "pos = lm.Classifier(name=\"pos\")\n",
    "pos.predict(d)\n",
    "\n",
    "print d[\"sentences\"] # [[{\"token\":\"This\",\"POS\":\"NNP\"},...                \n",
    "\n",
    "sentiment = lm.Classifier(name=\"sentiment\")\n",
    "sentiment.predict(d)\n",
    "print d[\"sentences\"] # [{\"sentiment\":\"Positive\",\"tokens\":[{\"token\":\"This\",\"POS\":\"NNP\"},..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Creating Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a Segmenter (tokenizer)\n",
    "contents = {}\n",
    "contents[\"text\"] = \"I like pie.\"\n",
    "contents[\"char_segs\"] = \"BOBIIIOBIIB\"\n",
    "d = lm.Text(contents)\n",
    "\n",
    "feats=[[\"text_-2\",\"text_-1\"],\"text_-1\",\"text_0\",\"text_1\",[\"text_1\",\"text_2\"]]\n",
    "\n",
    "ambiguity_hash = lm.AmbiguityHash(d,feats,\"char_segs\",threshold=0.95,min_count=5)\n",
    "\n",
    "percept_model = lm.ml.Sequential()\n",
    "percept_model.add(lm.ml.Linear(size=[d.getVocab(\"text\").size() * 10],encoding=\"hashtable\"))\n",
    "percept_model.add(lm.ml.LogSoftmax())\n",
    "\n",
    "percept_loss = lm.ml.PerceptronLoss()\n",
    "percept_optimizer = lm.ml.optim.PerceptronUpdate()\n",
    "percept_searcher = lm.ml.search.BeamSearch(beam=5)\n",
    "\n",
    "# where available in leau of calling all the forward/backprop logic from Python slowing things down\n",
    "tokenizer = lm.ml.trainer.FastSegmentationTrainer(d,\"char_segs\",feats,percept_model,percept_loss,percept_searcher,inherits=[ambiguity_hash],threads=50)\n",
    "\n",
    "tokenizer.segment(d,\"tokens\") #generates new vocab for what was segmented\n",
    "\n",
    "eos = lm.pretrained.Segmenter(name=\"eos\",lang=\"en\")\n",
    "eos.segment(d,\"sentences\",ignore_vocab=True) # does not generate a new vocab for what was segmented\n",
    "\n",
    "# https://github.com/oxford-cs-ml-2015/practical6/blob/master/train.lua\n",
    "lstm_hidden = 50\n",
    "seq_length = 16\n",
    "\n",
    "layers = list()\n",
    "for i in range(seq_length):\n",
    "    layer = {}\n",
    "    \n",
    "    inputs = {}\n",
    "    layer['embed'] = lm.ml.Embedding(d.getVocab(\"token\").size(),lstm_hidden)\n",
    "    layer['lstm'] = lm.ml.LSTM(lstm_hidden,input_x=layer['embed'],input_h=layers[i-1]['lstm']['h'])\n",
    "    layer['softmax'] = lm.ml.LogElasticHierarchicalSoftmax(input=layer['lstm']['h'],size=(lstm_hidden,d.getVocab(\"token\").size()),sample_rate=0.001)\n",
    "    layer['criterion'] = lm.ml.LogElasticHierarchicalSoftmaxLoss()\n",
    "    layers.append(layer)\n",
    "\n",
    "word_language_model_searcher = lm.ml.search.BeamSearch(beam=5)\n",
    "percept_optimizer = lm.ml.optim.SGD()\n",
    "word_language_model = lm.ml.trainer.FastLanguageModelTrainer(d,layers,word_language_model_searcher,percept_optimizer,predict_on=\"token\",bound_on=\"sentence\",threads=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ML Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ambiguity Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Neural Index"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
