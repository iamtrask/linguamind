{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import linguamind as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NLP Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Types of Component:\n",
    "- word level\n",
    "- sentence level\n",
    "- document level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = lm.Corpus(\"/my/folder/of/files\") # atempts to autodetect file type\n",
    "c = lm.Corpus(path=\"myfile.txt\",type=\"raw\")\n",
    "c = lm.Corpus(path=\"myfile.txt\",type=\"csv-token\") # one token per line in CSV\n",
    "c = lm.Corpus(path=\"myfile.txt\",type=\"csv-segment\") # text segment per line in CSV\n",
    "c = lm.Corpus(path=\"myfile.txt\",type=\"prefix__label__\") # one sentence per line with special labels\n",
    "c = lm.Corpus(path=\"myfile.txt\",type=\"json\")\n",
    "\n",
    "# document level raw text\n",
    "d = lm.Text(\"This is my sentence.\")\n",
    "d = lm.Text(\"myfile.txt\")\n",
    "\n",
    "# document level text with labels\n",
    "contents = {}\n",
    "contents[\"text\"] = \"This is my sentence. This is another one.\"\n",
    "contents[\"date\"] = 1997\n",
    "d = lm.Text(contents)\n",
    "\n",
    "# sentence level text with labels\n",
    "contents = {}\n",
    "contents[\"text\"] = \"This is my sentence.\"\n",
    "contents[\"sentiment\"] = \"Positive\"\n",
    "contents[\"speaker\"] = \"John Locke\"\n",
    "d = lm.Text(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok = lm.Segmenter(name=\"tokenizer\",lang=\"en\")\n",
    "eos = lm.Segmenter(name=\"eos\",lang=\"en\")\n",
    "\n",
    "# document level text with labels\n",
    "contents = {}\n",
    "contents[\"text\"] = \"This is my sentence. This is another one.\"\n",
    "contents[\"date\"] = 1997\n",
    "d = lm.Text(contents)\n",
    "\n",
    "tok.segment(d)\n",
    "eos.segment(d)\n",
    "\n",
    "print d[\"text\"] # \"This is my sentence. This is another one.\"\n",
    "print d[\"tokens\"] # [\"This\", \"is\"...\n",
    "print d[\"sentences\"] # [[\"This\", \"is\", \"my\", \"sentence\",\".\"],[\"This\",\"is\",\"another\",\"one\",\".\"]]                \n",
    "                     \n",
    "pos = lm.Classifier(name=\"pos\")\n",
    "pos.predict(d)\n",
    "\n",
    "print d[\"sentences\"] # [[{\"token\":\"This\",\"POS\":\"NNP\"},...                \n",
    "\n",
    "sentiment = lm.Classifier(name=\"sentiment\")\n",
    "sentiment.predict(d)\n",
    "print d[\"sentences\"] # [{\"sentiment\":\"Positive\",\"tokens\":[{\"token\":\"This\",\"POS\":\"NNP\"},..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Creating Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a Segmenter (tokenizer)\n",
    "contents = {}\n",
    "contents[\"text\"] = \"I like pie.\"\n",
    "contents[\"char_segs\"] = \"BOBIIIOBIIB\"\n",
    "d = lm.Text(contents)\n",
    "\n",
    "feats=[[\"text_-2\",\"text_-1\"],\"text_-1\",\"text_0\",\"text_1\",[\"text_1\",\"text_2\"]]\n",
    "\n",
    "ambiguity_hash = lm.AmbiguityHash(d,feats,\"char_segs\",threshold=0.95,min_count=5)\n",
    "\n",
    "percept_model = lm.ml.Sequential()\n",
    "percept_model.add(lm.ml.Linear(size=[d.getVocab(\"text\").size() * 10],encoding=\"hashtable\"))\n",
    "percept_model.add(lm.ml.LogSoftmax())\n",
    "\n",
    "percept_loss = lm.ml.PerceptronLoss()\n",
    "percept_optimizer = lm.ml.optim.PerceptronUpdate()\n",
    "percept_searcher = lm.ml.search.BeamSearch(beam=5)\n",
    "\n",
    "# where available in leau of calling all the forward/backprop logic from Python slowing things down\n",
    "percept_trainer = lm.ml.trainer.FastPerceptronTrainer(d,\"char_segs\",feats,percept_model,percept_loss,percept_searcher,inherits=[ambiguity_hash],threads=50)\n",
    "percept_trainer.train()\n",
    "\n",
    "# https://github.com/oxford-cs-ml-2015/practical6/blob/master/train.lua\n",
    "lstm_model = lm.ml.Sequential()\n",
    "lstm_model.add()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ML Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ambiguity Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Neural Index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
