{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import linguamind.linalg as la\n",
    "import linguamind.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = la.Seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SparseLinearInput():\n",
    "    \n",
    "    def __init__(self,input_dim, output_dim):\n",
    "        \n",
    "        self.sparse_output = False\n",
    "        \n",
    "        self.input_dim = input_dim # vocab size\n",
    "        self.output_dim = output_dim # embedding dim\n",
    "        \n",
    "        self.weights = la.Matrix(input_dim,output_dim)\n",
    "        \n",
    "        self.bias = la.Vector(output_dim)\n",
    "        self.bias.zero()\n",
    "        \n",
    "        self.input_indices = list(range(self.input_dim))\n",
    "        \n",
    "        self.output = la.Vector(output_dim).zero()\n",
    "        \n",
    "    def updateOutput(self, input_indices):\n",
    "        self.input_indices = input_indices\n",
    "        \n",
    "        self.output.zero()\n",
    "        for index in self.input_indices:\n",
    "            self.output += self.weights[index]\n",
    "            \n",
    "    def accGradParameters(self, input, output_grad, alpha):\n",
    "        # input is all 1s and is not used\n",
    "        for index in self.input_indices:\n",
    "            self.weights[index].addi(output_grad,-alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SparseLinearOutput():\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \n",
    "        self.sparse_output = True\n",
    "        \n",
    "        self.input_dim = input_dim # embedding dim\n",
    "        self.output_dim = output_dim # output vocab size \n",
    "        \n",
    "        self.weights = la.Matrix(output_dim,input_dim)\n",
    "        \n",
    "        self.output = la.Vector(output_dim).zero()\n",
    "        self.output_indices = list(range(self.output_dim))\n",
    "        self.input_grad = la.Vector(self.input_dim).zero()\n",
    "    \n",
    "    def updateOutput(self, input, output_indices=None):\n",
    "        if(output_indices is None):\n",
    "            output_indices = list(range(self.output_dim))\n",
    "        self.output_indices = output_indices\n",
    "        \n",
    "        for index in self.output_indices:\n",
    "            self.output.doti(index,input, self.weights[index])\n",
    "            \n",
    "    def updateInputGrad(self,output_grad):\n",
    "        self.input_grad.set(self.weights[self.output_indices[0]],output_grad[self.output_indices[0]]) \n",
    "        for index in self.output_indices[1:]:\n",
    "            self.input_grad.addi(self.weights[index],output_grad[index]) \n",
    "    \n",
    "    def accGradParameters(self, input, output_grad, alpha):\n",
    "        for index in self.output_indices:\n",
    "            self.weights[index].addi(input,output_grad[index] * -alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \n",
    "        self.sparse_output = False\n",
    "        \n",
    "        self.input_dim = input_dim # embedding dim\n",
    "        self.output_dim = output_dim # output vocab size \n",
    "        \n",
    "        self.weights = la.Matrix(output_dim,input_dim)\n",
    "        \n",
    "        self.output = la.Vector(output_dim).zero()\n",
    "        self.input_grad = la.Vector(self.input_dim).zero()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        for index in range(self.output_dim):\n",
    "            self.output.doti(index,input, self.weights[index])\n",
    "    \n",
    "    def updateInputGrad(self, output_grad):\n",
    "        self.input_grad.set(self.weights[0],output_grad[0]) \n",
    "        for index in range(self.output_dim-1):\n",
    "            self.input_grad.addi(self.weights[index+1],output_grad[index+1])\n",
    "            \n",
    "    def accGradParameters(self, input, output_grad, alpha):\n",
    "        for index in range(self.output_dim):\n",
    "            self.weights[index].addi(input,output_grad[index] * -alpha)\n",
    "\n",
    "class Relu():\n",
    "    def __init__(self, dim):\n",
    "        self.resize(dim)\n",
    "        self.sparse_output = False\n",
    "    \n",
    "    def resize(self,dim):\n",
    "        self.input_dim = dim\n",
    "        self.output_dim = dim\n",
    "        \n",
    "        self.weights = None\n",
    "        self.output_indices = list(range(dim))\n",
    "        \n",
    "        self.output = la.Vector(self.output_dim).zero()\n",
    "        self.input_grad = la.Vector(self.input_dim).zero()\n",
    "    \n",
    "    def updateOutput(self, input, output_indices = None):\n",
    "\n",
    "        if(output_indices is None):\n",
    "            output_indices = list(range(self.output_dim))\n",
    "        self.output_indices = output_indices\n",
    "        \n",
    "        self.input_grad *= 0\n",
    "        self.input_grad += input\n",
    "        self.input_grad >= 0\n",
    "        \n",
    "        self.output *= 0\n",
    "        self.output += input\n",
    "        self.output *= self.input_grad\n",
    "        \n",
    "    def updateInputGrad(self, output_grad):\n",
    "        self.input_grad *= output_grad\n",
    "        \n",
    "    def accGradParameters(self,alpha, input, output_grad):\n",
    "        \"do nothing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSECriterion():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\n",
    "        self.grad = la.Vector(32).zero()\n",
    "        \n",
    "    def forward(self,input,target,output_indices):\n",
    "        self.output = input\n",
    "        self.tmp_error = la.Vector(len(output_indices)).zero()\n",
    "        for i,index in enumerate(output_indices):\n",
    "            self.tmp_error[i] = self.output[index] - target[i]\n",
    "            \n",
    "        self.error = 0\n",
    "        for i in range(len(output_indices)):\n",
    "            self.error += self.tmp_error[i] * self.tmp_error[i]\n",
    "        self.error /= len(self.tmp_error)\n",
    "        \n",
    "        return self.error\n",
    "    \n",
    "    def backward(self,output, target,output_indices):\n",
    "        if(self.grad.size != output.size):\n",
    "            self.grad = la.Vector(output.size).zero()\n",
    "        for i,index in enumerate(output_indices):\n",
    "            self.grad[index] = output[index] - target[i]\n",
    "#             print(str(output[index]) + \" - \" + str(target[i]))\n",
    "        return self.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Sequential():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\n",
    "        self.layers = list()\n",
    "    \n",
    "    def add(self,layer):\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "        self.output = self.layers[-1].output\n",
    "    \n",
    "    def forward(self,input_indices=(1,3,2), output_indices=(1,2,4)):\n",
    "        \n",
    "        self.layers[0].updateOutput(input_indices)\n",
    "        \n",
    "        sparse_output_until_end = False\n",
    "        \n",
    "        for index in range(len(self.layers)-2):\n",
    "            if(self.layers[index+1].sparse_output):\n",
    "                sparse_output_until_end = True\n",
    "                \n",
    "            if(sparse_output_until_end):\n",
    "                self.layers[index+1].updateOutput(self.layers[index].output,output_indices)\n",
    "            else:\n",
    "                self.layers[index+1].updateOutput(self.layers[index].output)\n",
    "            \n",
    "        self.layers[-1].updateOutput(self.layers[-2].output,output_indices)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad, output_indices):\n",
    "        \n",
    "        self.layers[-1].updateInputGrad(grad)\n",
    "        sparse_output_until_end = True\n",
    "        for i in reversed(range(len(self.layers)-2)):\n",
    "            if(sparse_output_until_end):\n",
    "                self.layers[i+1].updateInputGrad(self.layers[i+2].input_grad)\n",
    "            else:\n",
    "                self.layers[i+1].updateInputGrad(self.layers[i+2].input_grad)\n",
    "            if(self.layers[i+1].sparse_output == True):\n",
    "                sparse_output_until_end = False\n",
    "                \n",
    "class StochasticGradient():\n",
    "    \n",
    "    def __init__(self,mlp,criterion,alpha = 0.01):\n",
    "        self.mlp = mlp\n",
    "        self.criterion = criterion\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def train(self,input_indices, output_indices, target_values): \n",
    "        \n",
    "        pred = self.mlp.forward(input_indices,output_indices)\n",
    "        error = self.criterion.forward(pred,target_values,output_indices)\n",
    "        self.mlp.backward(self.criterion.backward(pred,target_values,output_indices),output_indices)\n",
    "        \n",
    "        # update weights for sparse layer\n",
    "        mlp.layers[0].accGradParameters(None,mlp.layers[1].input_grad,self.alpha)\n",
    "        \n",
    "        # update dense middle layers\n",
    "        for i in range(len(self.mlp.layers)-2):\n",
    "            mlp.layers[i+1].accGradParameters(mlp.layers[i].output,mlp.layers[i+2].input_grad,self.alpha)\n",
    "                    \n",
    "        # update weights for output with criterion gradient        \n",
    "        mlp.layers[-1].accGradParameters(mlp.layers[-2].output,criterion.grad,self.alpha)\n",
    "            \n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<linguamind.linalg.Matrix; proxy of <Swig Object of type 'Matrix *' at 0x1049a1f60> >"
      ]
     },
     "execution_count": 878,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = la.Seed(1)\n",
    "\n",
    "syn0 = SparseLinearInput(100,64)\n",
    "syn0.weights.uniform(seed)\n",
    "syn0.weights -= 0.5\n",
    "syn0.weights /= 50\n",
    "\n",
    "syn1 = Linear(64,32)\n",
    "syn1.weights.uniform(seed)\n",
    "syn1.weights -= 0.5\n",
    "syn1.weights /= 50\n",
    "\n",
    "syn2 = Linear(32,16)\n",
    "syn2.weights.uniform(seed)\n",
    "syn2.weights -= 0.5\n",
    "syn2.weights /= 50\n",
    "\n",
    "syn3 = SparseLinearOutput(16,100)\n",
    "syn3.weights.uniform(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = Sequential()\n",
    "\n",
    "mlp.add(syn0)\n",
    "mlp.add(syn1)\n",
    "mlp.add(Relu(32))\n",
    "mlp.add(syn2)\n",
    "mlp.add(Relu(16))\n",
    "mlp.add(syn3)\n",
    "mlp.add(Relu(100))\n",
    "\n",
    "criterion = MSECriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_indices = (1,2,3,4)\n",
    "output_indices = (1,2,3,4,5,6,20,4)\n",
    "\n",
    "target = la.Vector(len(output_indices))\n",
    "target.zero()\n",
    "target.set(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optim = StochasticGradient(mlp,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499696034424791\n",
      "0.12499694544474545\n",
      "0.12499693054525286\n",
      "0.12499691564577009\n",
      "0.1249969007462974\n",
      "0.12499688584683458\n",
      "0.1249968709473818\n",
      "0.12499684114696578\n",
      "0.12499682624753408\n",
      "0.12499681134811254\n",
      "0.12499679644870126\n",
      "0.12499678154930016\n",
      "0.12499676664990929\n",
      "0.12499675175052874\n",
      "0.1249967368511587\n",
      "0.12499672195179899\n",
      "0.12499670705244975\n",
      "0.12499669215311102\n",
      "0.12499667725378279\n",
      "0.12499666235446516\n",
      "0.1249966474551583\n",
      "0.12499663255586202\n",
      "0.12499660275561736\n",
      "0.12499658785634356\n",
      "0.12499657295708051\n",
      "0.12499655805782837\n",
      "0.12499654315858721\n",
      "0.12499652825935705\n",
      "0.12499651336013776\n",
      "0.1249964984609296\n",
      "0.1249964835617326\n",
      "0.12499646866254684\n",
      "0.12499645376337223\n",
      "0.12499642396326044\n",
      "0.1249964090641093\n",
      "0.1249963941649695\n",
      "0.12499637926584113\n",
      "0.12499636436672437\n",
      "0.12499634946761892\n",
      "0.1249963345685253\n",
      "0.12499631966944322\n",
      "0.1249963047703727\n",
      "0.1249962749703745\n",
      "0.12499626007132844\n",
      "0.12499624517229432\n",
      "0.12499623027327217\n",
      "0.12499621537426182\n",
      "0.12499620047526354\n",
      "0.12499618557627733\n",
      "0.12499617067730326\n",
      "0.12499614087740975\n",
      "0.124996125978461\n",
      "0.1249961110795244\n",
      "0.12499609618060034\n",
      "0.12499608128168863\n",
      "0.12499606638278926\n",
      "0.1249960514839026\n",
      "0.12499603658502849\n",
      "0.12499600678524328\n",
      "0.12499599188639536\n",
      "0.12499597698756025\n",
      "0.12499596208873791\n",
      "0.1249959471899284\n",
      "0.1249959322911318\n",
      "0.12499591739232514\n",
      "0.1249958875923119\n",
      "0.1249958726932572\n",
      "0.12499585779422169\n",
      "0.12499584289519737\n",
      "0.12499582799618446\n",
      "0.1249958130971831\n",
      "0.12499579819818196\n",
      "0.12499578329916759\n",
      "0.12499576840020055\n",
      "0.12499575350124524\n",
      "0.12499573860230173\n",
      "0.12499572370336992\n",
      "0.1249957088044501\n",
      "0.1249956939054922\n",
      "0.12499567900659747\n",
      "0.12499566410771339\n",
      "0.12499564920884113\n",
      "0.12499563430998141\n",
      "0.12499560451023399\n",
      "0.12499558961137303\n",
      "0.12499557471252737\n",
      "0.12499555981371743\n",
      "0.12499554491492\n",
      "0.124995530016135\n",
      "0.12499551511736272\n",
      "0.12499550021859872\n",
      "0.12499548531980463\n",
      "0.12499547042107055\n",
      "0.12499545552234895\n",
      "0.12499544062364039\n",
      "0.12499541082405684\n",
      "0.12499539592537538\n",
      "0.12499538102665356\n",
      "0.12499536612799857\n",
      "0.1249953512293563\n",
      "0.12499533633072756\n",
      "0.12499532143211194\n",
      "0.12499530653351014\n",
      "0.12499529163488673\n",
      "0.12499526183541323\n",
      "0.12499524693685284\n",
      "0.1249952320383061\n",
      "0.12499521713977334\n",
      "0.12499520224125436\n",
      "0.12499518734273098\n",
      "0.12499517244420266\n",
      "0.1249951575457254\n",
      "0.12499512774639167\n",
      "0.12499511284794408\n",
      "0.12499509794951082\n",
      "0.12499508305108832\n",
      "0.12499506815263009\n",
      "0.12499505325424014\n",
      "0.12499502345499958\n",
      "0.12499500855663995\n",
      "0.12499499365829518\n",
      "0.12499497875996508\n",
      "0.12499496386158208\n",
      "0.12499494896329058\n",
      "0.12499493406500556\n",
      "0.1249949042658777\n",
      "0.12499488936762397\n",
      "0.12499487446938562\n",
      "0.12499485957110426\n",
      "0.12499484467289444\n",
      "0.12499481487384986\n",
      "0.12499479997567428\n",
      "0.12499478507751478\n",
      "0.12499477017937108\n",
      "0.12499475528119205\n",
      "0.12499472548222178\n",
      "0.12499471058412674\n",
      "0.12499469568604782\n",
      "0.12499468078798523\n",
      "0.12499466588993914\n",
      "0.12499463609102136\n",
      "0.12499462119299128\n",
      "0.12499460629499534\n",
      "0.12499459139701617\n",
      "0.12499457649905382\n",
      "0.12499454670027187\n",
      "0.12499453180230082\n",
      "0.12499451690436847\n",
      "0.12499450200647488\n",
      "0.12499447220782484\n",
      "0.12499444241067102\n",
      "0.12499439771272694\n",
      "0.12499435301482527\n",
      "0.12499430831696591\n",
      "0.12499427851996885\n",
      "0.12499423382219217\n",
      "0.12499418912443211\n",
      "0.12499414442669587\n",
      "0.12499411462985878\n",
      "0.12499406993225265\n",
      "0.12499402523469065\n",
      "0.12499398053717295\n",
      "0.12499395074049999\n",
      "0.12499390604306873\n",
      "0.1249938613456829\n",
      "0.12499381664834192\n",
      "0.12499377195104695\n",
      "0.12499374215458535\n",
      "0.12499369745737925\n",
      "0.12499365276021984\n",
      "0.12499360806313177\n",
      "0.1249935633666172\n",
      "0.12499351867015726\n",
      "0.12499347397375253\n",
      "0.12499342927740309\n",
      "0.12499338458110913\n",
      "0.12499333988484404\n",
      "0.1249932802878384\n",
      "0.12499323559171568\n",
      "0.12499319089564999\n",
      "0.12499314619964164\n",
      "0.12499310150369096\n",
      "0.12499305680779846\n",
      "0.12499301211196454\n",
      "0.12499295251544781\n",
      "0.12499290781973431\n",
      "0.12499286312408096\n",
      "0.12499281842848683\n",
      "0.1249927737329533\n",
      "0.1249927141367532\n",
      "0.12499266944134423\n",
      "0.12499262474599643\n",
      "0.12499258005071091\n",
      "0.12499252045477162\n",
      "0.12499247575961356\n",
      "0.12499243106451792\n",
      "0.1249923863694864\n",
      "0.12499232677381457\n",
      "0.12499228207891351\n",
      "0.12499223738403897\n",
      "0.12499219268919956\n",
      "0.12499213309380044\n",
      "0.1249920883991623\n",
      "0.12499204370459067\n",
      "0.12499198410940202\n",
      "0.12499193941496724\n",
      "0.12499189472059996\n",
      "0.12499183512562578\n",
      "0.12499179043139803\n",
      "0.12499174573723887\n",
      "0.12499168614248367\n",
      "0.12499164144846715\n",
      "0.12499159675452107\n",
      "0.12499153715998848\n",
      "0.12499149246618693\n",
      "0.12499143287180672\n",
      "0.12499138817815166\n",
      "0.12499132858392556\n",
      "0.12499128389041966\n",
      "0.12499123919698754\n",
      "0.1249911796029367\n",
      "0.1249911349095915\n",
      "0.12499107531575604\n",
      "0.12499103062262895\n",
      "0.12499097102895422\n",
      "0.12499092633598241\n",
      "0.1249908667424712\n",
      "0.124990822049658\n",
      "0.1249907624563119\n",
      "0.12499071776365836\n",
      "0.12499065817047969\n",
      "0.12499061347798844\n",
      "0.1249905538849795\n",
      "0.12499049429205798\n",
      "0.12499044959981588\n",
      "0.1249903900070673\n",
      "0.12499034531499333\n",
      "0.12499028572242017\n",
      "0.12499022612993706\n",
      "0.12499018143807362\n",
      "0.12499012184567859\n",
      "0.1249900622534654\n",
      "0.12499001756191085\n",
      "0.12498995796988012\n",
      "0.12498989837794396\n",
      "0.12498985368665785\n",
      "0.12498979409490774\n",
      "0.12498973450325264\n",
      "0.1249896898122407\n",
      "0.1249896302207769\n",
      "0.12498957062941032\n",
      "0.12498951103814211\n",
      "0.12498946634750599\n",
      "0.12498940675643312\n",
      "0.12498934716546059\n",
      "0.12498928757458921\n",
      "0.12498922798381998\n",
      "0.1249891832936076\n",
      "0.12498912370294749\n",
      "0.12498906411248262\n",
      "0.12498900452212239\n",
      "0.12498894493186806\n",
      "0.12498888534171931\n",
      "0.12498882575167809\n",
      "0.12498876616174359\n",
      "0.1249887065719182\n",
      "0.12498866188268579\n",
      "0.12498860229307569\n",
      "0.12498854270357566\n",
      "0.12498848311418755\n",
      "0.1249884235249107\n",
      "0.12498836393574707\n",
      "0.1249883043466974\n",
      "0.12498824475776249\n",
      "0.12498818516885554\n",
      "0.12498812558006595\n",
      "0.12498806599147737\n",
      "0.12498800640300664\n",
      "0.12498793191421291\n",
      "0.12498787232598421\n",
      "0.1249878127378756\n",
      "0.12498775314988816\n",
      "0.12498769356202293\n",
      "0.12498763397428028\n",
      "0.12498757438666193\n",
      "0.12498751479916792\n",
      "0.12498744031138675\n",
      "0.12498738072414815\n",
      "0.1249873211370377\n",
      "0.12498726155005475\n",
      "0.12498718706269947\n",
      "0.12498712747589419\n",
      "0.12498706788929334\n",
      "0.12498700830283789\n",
      "0.12498693381613314\n",
      "0.12498687422994813\n",
      "0.1249868146438998\n",
      "0.1249867401576155\n",
      "0.12498668057184398\n",
      "0.1249866060858477\n",
      "0.12498654650035705\n",
      "0.12498648691500693\n",
      "0.1249864124294469\n",
      "0.12498635284438459\n",
      "0.12498627835912231\n",
      "0.12498621877424632\n",
      "0.12498611449673885\n",
      "0.12498602512004003\n",
      "0.12498592084328139\n",
      "0.12498583146711412\n",
      "0.12498572719090206\n",
      "0.12498563781527515\n",
      "0.12498553353961675\n",
      "0.1249854292640121\n",
      "0.12498533988909591\n",
      "0.12498523561428472\n",
      "0.12498514624003791\n",
      "0.12498504196580151\n",
      "0.12498493769185667\n",
      "0.12498483341820897\n",
      "0.12498474404504707\n",
      "0.12498463977168581\n",
      "0.1249845354989244\n",
      "0.12498444612670039\n",
      "0.12498434185454152\n",
      "0.12498423758269155\n",
      "0.12498413331115117\n",
      "0.12498402903992305\n",
      "0.12498392476876513\n",
      "0.12498382049802557\n",
      "0.12498371622774246\n",
      "0.12498362685796678\n",
      "0.12498352258832131\n",
      "0.12498341831900309\n",
      "0.12498331405001206\n",
      "0.12498320978124534\n",
      "0.12498310551261962\n",
      "0.12498300124462355\n",
      "0.12498288207682454\n",
      "0.12498277780951375\n",
      "0.12498267354254348\n",
      "0.12498256927591926\n",
      "0.12498246500964312\n",
      "0.12498236074334598\n",
      "0.12498224157761412\n",
      "0.12498213731239873\n",
      "0.12498203304754123\n",
      "0.12498192878304666\n",
      "0.12498180961883691\n",
      "0.12498170535507688\n",
      "0.12498160109140373\n",
      "0.12498148192816196\n",
      "0.1249813776655212\n",
      "0.12498127340325628\n",
      "0.12498115424133457\n",
      "0.12498104997984075\n",
      "0.12498093081870751\n",
      "0.1249808265577806\n",
      "0.1249807073971954\n",
      "0.12498060313727226\n",
      "0.1249804839777508\n",
      "0.12498036481864343\n",
      "0.12498026055993681\n",
      "0.12498014140166235\n",
      "0.12498002224364113\n",
      "0.12497991798585921\n",
      "0.12497979882885583\n",
      "0.1249796796722834\n",
      "0.12497956051612699\n",
      "0.12497945626046333\n",
      "0.12497933710530297\n",
      "0.1249792179504851\n",
      "0.12497909879581802\n",
      "0.1249789796420119\n",
      "0.12497886048866445\n",
      "0.12497874133578121\n",
      "0.1249786221833646\n",
      "0.12497850303141812\n",
      "0.12497836898006251\n",
      "0.12497824982854926\n",
      "0.12497813067803953\n",
      "0.12497801152801534\n",
      "0.12497789237848093\n",
      "0.12497777322944226\n",
      "0.12497763918106987\n",
      "0.12497752003303493\n",
      "0.12497740088505761\n",
      "0.12497726683811172\n",
      "0.12497714769160685\n",
      "0.12497701364582976\n",
      "0.12497689450037262\n",
      "0.12497676045566265\n",
      "0.12497664131127104\n",
      "0.1249765072672195\n",
      "0.12497638812374134\n",
      "0.12497625408122047\n",
      "0.12497613493900196\n",
      "0.12497600089760294\n",
      "0.12497586685677779\n",
      "0.12497573281652873\n",
      "0.12497561367611977\n",
      "0.12497547963685453\n",
      "0.12497534559835062\n",
      "0.1249752115604396\n",
      "0.12497507752313122\n",
      "0.12497494348642796\n",
      "0.1249748094503331\n",
      "0.12497467541435657\n",
      "0.12497454137934702\n",
      "0.12497440734510783\n",
      "0.1249742733115014\n",
      "0.12497413927852859\n",
      "0.12497400524619807\n",
      "0.12497387121450959\n",
      "0.12497372228328889\n",
      "0.12497358825283518\n",
      "0.12497345422312167\n",
      "0.12497330529450583\n",
      "0.1249731712661439\n",
      "0.12497303723846051\n",
      "0.12497288831182371\n",
      "0.12497275428491078\n",
      "0.12497260535977248\n",
      "0.12497247133487412\n",
      "0.1249723224111706\n",
      "0.12497217348819734\n",
      "0.12497203946556404\n",
      "0.12497189054411309\n",
      "0.12497174162324065\n",
      "0.12497159270358889\n",
      "0.12497144378470659\n",
      "0.12497129486659275\n",
      "0.12497114594926172\n",
      "0.12497099703266835\n",
      "0.12497084811617507\n",
      "0.1249706992012151\n",
      "0.12497055028706386\n",
      "0.12497040137372324\n",
      "0.124970252461205\n",
      "0.12497010354951203\n",
      "0.1249699397386982\n",
      "0.1249697908284555\n",
      "0.12496964191929077\n",
      "0.1249694781116378\n",
      "0.1249693292042007\n",
      "0.12496916539831138\n",
      "0.12496901649223369\n",
      "0.1249688526876832\n",
      "0.12496868888449505\n",
      "0.12496853998150655\n",
      "0.12496837618015376\n",
      "0.12496821237973962\n",
      "0.12496804858003299\n",
      "0.12496789968010967\n",
      "0.12496773588253311\n",
      "0.12496757208592872\n",
      "0.12496740829029658\n",
      "0.12496724449565026\n",
      "0.12496708070191016\n",
      "0.12496691690844042\n",
      "0.12496673821873076\n",
      "0.12496652975318599\n",
      "0.12496632128913622\n",
      "0.12496611282660965\n",
      "0.1249658894664843\n",
      "0.12496568100704458\n",
      "0.12496547254867918\n",
      "0.12496524919321617\n",
      "0.12496502583943755\n",
      "0.12496481738635644\n",
      "0.12496459403584986\n",
      "0.12496437068661315\n",
      "0.12496414733927905\n",
      "0.1249639239938204\n",
      "0.12496370065008029\n",
      "0.1249634773080785\n",
      "0.1249632539674851\n",
      "0.12496303062874675\n",
      "0.12496280729205349\n",
      "0.1249625690582341\n",
      "0.12496234572518913\n",
      "0.12496210749477002\n",
      "0.1249618841651326\n",
      "0.12496164593880918\n",
      "0.12496140771441273\n",
      "0.12496116949197494\n",
      "0.1249609312711696\n",
      "0.12496069305236734\n",
      "0.12496045483588511\n",
      "0.12496021662143963\n",
      "0.12495997840904091\n",
      "0.1249597401983405\n",
      "0.12495948709107521\n",
      "0.12495924888496908\n",
      "0.12495899578228988\n",
      "0.124958757580507\n",
      "0.12495850448174174\n",
      "0.12495825138545807\n",
      "0.12495799829165545\n",
      "0.12495774520013835\n",
      "0.12495749211092998\n",
      "0.12495723902344996\n",
      "0.12495698593879781\n",
      "0.12495673285664982\n",
      "0.12495646487833914\n",
      "0.12495621180102391\n",
      "0.12495594382692268\n",
      "0.12495567585605499\n",
      "0.12495542278623185\n",
      "0.12495515482045565\n",
      "0.12495488685705072\n",
      "0.12495461889594428\n",
      "0.12495433603959194\n",
      "0.12495406808437579\n",
      "0.12495380013186372\n",
      "0.12495351728320761\n",
      "0.12495324933589841\n",
      "0.1249529664946134\n",
      "0.12495268367129282\n",
      "0.1249523859528426\n",
      "0.12495208823767544\n",
      "0.12495179052563343\n",
      "0.12495149281699636\n",
      "0.12495119511184223\n",
      "0.12495089741012438\n",
      "0.12495058481364318\n",
      "0.12495028711879341\n",
      "0.12494997452919\n",
      "0.12494966194340627\n",
      "0.12494934936127454\n",
      "0.12494903678284115\n",
      "0.12494872420808815\n",
      "0.12494841163689788\n",
      "0.12494809906980038\n",
      "0.12494777160850778\n",
      "0.1249474590491935\n",
      "0.12494713159579904\n",
      "0.12494680414617348\n",
      "0.12494649159889513\n",
      "0.1249461641577622\n",
      "0.12494582182287685\n",
      "0.12494549439009255\n",
      "0.12494516696138551\n",
      "0.12494482463939018\n",
      "0.12494448232182193\n",
      "0.12494415490654\n",
      "0.12494381259777218\n",
      "0.12494347029350518\n",
      "0.12494311309629982\n",
      "0.12494277080156498\n",
      "0.12494242851156082\n",
      "0.12494207132836664\n",
      "0.12494171415019982\n",
      "0.12494135697707823\n",
      "0.12494099980896502\n",
      "0.12494064264590295\n",
      "0.12494028548751357\n",
      "0.12493992833478447\n",
      "0.12493955628968997\n",
      "0.12493918424992619\n",
      "0.12493881221538974\n",
      "0.12493844018625695\n",
      "0.1249380681629037\n",
      "0.12493769614514097\n",
      "0.12493730923561018\n",
      "0.12493693722882347\n",
      "0.12493655033089437\n",
      "0.12493616343887796\n",
      "0.12493577655285479\n",
      "0.1249353896726411\n",
      "0.1249349879012713\n",
      "0.12493460103364352\n",
      "0.1249341992750453\n",
      "0.12493379752282757\n",
      "0.12493339577666339\n",
      "0.1249329940374872\n",
      "0.1249325923049663\n",
      "0.12493217568203348\n",
      "0.12493175906549858\n",
      "0.12493134245625381\n",
      "0.12493092585408169\n",
      "0.12493050925902714\n",
      "0.12493009267083675\n",
      "0.12492966119300747\n",
      "0.12492922972284559\n",
      "0.12492881315707156\n",
      "0.12492836680486802\n",
      "0.12492793535709171\n",
      "0.12492750391744789\n",
      "0.12492705758885403\n",
      "0.12492661126819815\n",
      "0.12492616495530726\n",
      "0.12492571865110022\n",
      "0.12492525745847992\n",
      "0.12492481117099223\n",
      "0.12492434999487331\n",
      "0.12492388882798798\n",
      "0.12492342766993296\n",
      "0.12492295162416192\n",
      "0.12492249048343598\n",
      "0.12492201445593867\n",
      "0.12492153843772791\n",
      "0.12492106242892712\n",
      "0.12492057153263472\n",
      "0.12492009554306951\n",
      "0.12491960466691636\n",
      "0.1249191138006653\n",
      "0.12491862294394544\n",
      "0.12491811720177973\n",
      "0.1249176114700052\n",
      "0.12491710574863285\n",
      "0.12491660003750277\n",
      "0.12491609433773357\n",
      "0.12491557375276831\n",
      "0.12491506807482859\n",
      "0.12491454751177433\n",
      "0.1249140120645835\n",
      "0.1249134915250629\n",
      "0.12491295610089109\n",
      "0.12491242068869116\n",
      "0.12491188528894073\n",
      "0.12491133500554572\n",
      "0.12491079963060897\n",
      "0.12491024937347596\n",
      "0.12490968423367826\n",
      "0.12490913399890141\n",
      "0.12490856888277063\n",
      "0.12490800378257956\n",
      "0.12490743869589956\n",
      "0.12490687362271632\n",
      "0.12490629366160674\n",
      "0.12490571372143235\n",
      "0.12490513379534518\n",
      "0.12490453898759044\n",
      "0.12490395909031675\n",
      "0.12490336430746923\n",
      "0.12490275464982441\n",
      "0.12490215990228501\n",
      "0.12490155027592158\n",
      "0.12490094066323133\n",
      "0.12490031616908555\n",
      "0.1248997065906154\n",
      "0.12489908213387035\n",
      "0.12489844279882484\n",
      "0.12489781837054974\n",
      "0.12489717906808558\n",
      "0.12489653978498146\n",
      "0.12489588562418258\n",
      "0.12489523148161183\n",
      "0.12489457735129152\n",
      "0.12489392324619333\n",
      "0.12489325426409591\n",
      "0.12489258530226764\n",
      "0.1248919014651578\n",
      "0.12489123253502137\n",
      "0.12489054873736717\n",
      "0.12488985006582635\n",
      "0.1248891663093942\n",
      "0.1248884676762204\n",
      "0.12488775416852235\n",
      "0.12488705558150069\n",
      "0.12488632722661076\n",
      "0.12488561378917222\n",
      "0.12488488547569779\n",
      "0.12488415718646186\n",
      "0.12488342892296148\n",
      "0.12488268578969611\n",
      "0.12488194268045176\n",
      "0.12488118469326985\n",
      "0.12488042673837457\n",
      "0.12487966880928558\n",
      "0.12487889601076783\n",
      "0.12487812323867958\n",
      "0.12487735048379066\n",
      "0.12487656287249736\n",
      "0.12487577528613657\n",
      "0.12487497283454103\n",
      "0.1248741704096791\n",
      "0.12487336800601288\n",
      "0.12487255074611021\n",
      "0.1248717186218862\n",
      "0.12487090141990191\n",
      "0.12487006935147779\n",
      "0.12486922241986183\n",
      "0.12486837552338854\n",
      "0.12486752865843043\n",
      "0.12486666693320395\n",
      "0.12486580523455763\n",
      "0.12486492867782104\n",
      "0.12486405215898089\n",
      "0.12486317567436776\n",
      "0.12486226943714016\n",
      "0.1248613781209136\n",
      "0.12486047195580881\n",
      "0.12485956582662544\n",
      "0.1248586448422577\n",
      "0.12485770900338211\n",
      "0.12485678808385048\n",
      "0.1248558374298581\n",
      "0.12485488681506034\n",
      "0.12485393624008329\n",
      "0.12485297081158354\n",
      "0.12485200541737919\n",
      "0.12485102518213927\n",
      "0.12485004498781828\n",
      "0.12484904994672966\n",
      "0.12484804005513006\n",
      "0.12484703020163211\n",
      "0.12484602040278343\n",
      "0.12484499575868221\n",
      "0.12484395626861282\n",
      "0.12484291682233958\n",
      "0.12484186253235649\n",
      "0.12484080829696821\n",
      "0.12483973922006701\n",
      "0.12483867019543211\n",
      "0.12483758632450233\n",
      "0.12483648761546201\n",
      "0.12483538896658682\n",
      "0.12483427548084382\n",
      "0.12483314716681806\n",
      "0.1248320040722683\n",
      "0.12483084615322304\n",
      "0.12482967340593593\n",
      "0.1248285007190651\n",
      "0.12482731319783118\n",
      "0.1248261108462657\n",
      "0.12482490856752634\n",
      "0.12482367657259831\n",
      "0.1248224595345236\n",
      "0.12482121276358085\n",
      "0.12481996608441898\n",
      "0.12481870458102534\n",
      "0.12481744314860177\n",
      "0.12481616688617013\n",
      "0.12481487581432785\n",
      "0.12481356993184067\n",
      "0.12481226412543872\n",
      "0.124810943501478\n",
      "0.12480960806067673\n",
      "0.12480825781930636\n",
      "0.12480690766074869\n",
      "0.12480554269572235\n",
      "0.1248041629075528\n",
      "0.12480276833536971\n",
      "0.12480137384746647\n",
      "0.12479994967086332\n",
      "0.12479852557568985\n",
      "0.12479708668558279\n",
      "0.12479563300753024\n",
      "0.124794179423343\n",
      "0.12479269615961071\n",
      "0.12479121297284597\n",
      "0.12478971502485976\n",
      "0.12478818739738251\n",
      "0.12478665987254589\n",
      "0.12478513244293517\n",
      "0.12478357535097014\n",
      "0.12478200348680826\n",
      "0.12478041684662596\n",
      "0.12477883031796677\n",
      "0.12477721411332478\n",
      "0.12477559804518568\n",
      "0.12477395232208809\n",
      "0.12477230672119304\n",
      "0.12477063145568622\n",
      "0.12476895632492184\n",
      "0.12476725155561585\n",
      "0.12476554691819591\n",
      "0.1247638126399238\n",
      "0.1247620784731855\n",
      "0.12476031470181408\n",
      "0.12475853618130342\n",
      "0.12475674291880527\n",
      "0.12475494979269386\n",
      "0.12475312704123813\n",
      "0.12475127468258235\n",
      "0.12474942247955134\n",
      "0.12474755554911529\n",
      "0.12474565898539716\n",
      "0.12474374772089164\n",
      "0.12474182174038811\n",
      "0.1247398810450596\n",
      "0.12473792563583712\n",
      "0.12473594061253271\n",
      "0.12473394090769076\n",
      "0.12473192650211554\n",
      "0.12472989739929956\n",
      "0.12472783870525285\n",
      "0.1247257653203974\n",
      "0.12472367726362124\n",
      "0.12472155964068879\n",
      "0.12471942733962588\n",
      "0.12471726545878173\n",
      "0.12471510381019489\n",
      "0.12471289773304861\n",
      "0.12471069188002995\n",
      "0.12470844160122718\n",
      "0.12470619152816037\n",
      "0.12470391195513718\n",
      "0.12470161773868957\n",
      "0.12469927911826784\n",
      "0.12469694074641674\n",
      "0.12469457284151851\n",
      "0.12469217545882803\n",
      "0.12468976346027148\n",
      "0.12468732196645196\n",
      "0.12468486586358599\n",
      "0.12468238026223857\n",
      "0.12467986520850742\n",
      "0.12467732068495446\n",
      "0.12467476157823104\n",
      "0.12467217300973304\n",
      "0.12466956985946129\n",
      "0.12466692240586423\n",
      "0.124664260394109\n",
      "0.12466156895501139\n",
      "0.12465884808516921\n",
      "0.12465609778829236\n",
      "0.12465333298539967\n",
      "0.124650523900063\n",
      "0.12464770030652607\n",
      "0.12464483244056514\n",
      "0.12464195006319088\n",
      "0.12463902345917886\n",
      "0.12463608237923986\n",
      "0.12463309707093147\n",
      "0.12463009729553556\n",
      "0.1246270532837638\n",
      "0.1246239799778597\n",
      "0.12462087736209063\n",
      "0.12461774544313245\n",
      "0.12461456935374826\n",
      "0.12461137881145064\n",
      "0.12460814417543478\n",
      "0.1246048653926942\n",
      "0.12460155736071135\n",
      "0.12459822008739528\n",
      "0.12459485355112603\n",
      "0.12459144295897294\n",
      "0.1245880031647318\n",
      "0.12458451930390012\n",
      "0.12458099138395974\n",
      "0.12457743428236306\n",
      "0.1245738480335056\n",
      "0.12457020291296782\n",
      "0.12456652866541813\n",
      "0.12456282530934411\n",
      "0.12455906310694773\n",
      "0.12455527179096408\n",
      "0.12455143657264776\n",
      "0.12454755742780584\n",
      "0.12454363438125084\n",
      "0.12453966744606579\n",
      "0.12453565660250264\n",
      "0.12453161679883737\n",
      "0.1245275182979569\n",
      "0.12452337598068282\n",
      "0.1245191749922168\n",
      "0.12451494508928361\n",
      "0.1245106713881522\n",
      "0.12450632424940512\n",
      "0.1245019482645642\n",
      "0.12449751371264782\n",
      "0.12449303547947144\n",
      "0.12448851357005475\n",
      "0.12448393317240897\n",
      "0.12447929431847722\n",
      "0.12447459701261117\n",
      "0.12446985614902574\n",
      "0.12446505689059419\n",
      "0.12446021408093978\n",
      "0.12445529810312629\n",
      "0.12445033867092764\n",
      "0.12444532095762512\n",
      "0.1244402301186768\n",
      "0.12443509591790751\n",
      "0.12442988858728626\n",
      "0.12442462315747367\n",
      "0.1244192995938188\n",
      "0.1244139179302062\n",
      "0.12440846333470701\n",
      "0.12440293583861728\n",
      "0.12439736514844064\n",
      "0.12439170685491806\n",
      "0.12438599063366161\n",
      "0.12438020166671584\n",
      "0.12437435485665697\n",
      "0.12436842052979843\n",
      "0.12436242838005061\n",
      "0.12435634886839012\n",
      "0.12435021167910078\n",
      "0.12434398714909761\n",
      "0.12433769018882397\n",
      "0.12433130598202212\n",
      "0.12432484938954744\n",
      "0.12431832054360291\n",
      "0.12431170463312219\n",
      "0.12430500168697127\n",
      "0.12429822662336731\n",
      "0.12429134977938792\n",
      "0.12428440090422133\n",
      "0.1242773652192035\n",
      "0.1242702428445944\n",
      "0.12426301894445721\n",
      "0.1242557084346275\n",
      "0.12424829653680876\n",
      "0.12424079818301784\n",
      "0.12423321336449193\n",
      "0.12422551259051351\n",
      "0.12421772557212643\n",
      "0.12420983754712446\n",
      "0.12420183374187628\n",
      "0.12419372909012699\n",
      "0.12418552367522603\n",
      "0.12417721750270376\n",
      "0.12416879599007613\n",
      "0.12416027399541182\n",
      "0.12415162191522905\n",
      "0.1241428695442942\n",
      "0.124133987278293\n",
      "0.12412499007296311\n",
      "0.12411589286911173\n",
      "0.12410665132321858\n",
      "0.12409729519402692\n",
      "0.12408780974113762\n",
      "0.12407819510760043\n",
      "0.12406845141970176\n",
      "0.12405857877281178\n",
      "0.12404856245885387\n",
      "0.1240384175432134\n",
      "0.12402812928256408\n",
      "0.12401771267030441\n",
      "0.12400713816801871\n",
      "0.12399642077534657\n",
      "0.12398554581162154\n",
      "0.12397454303181202\n",
      "0.12396336830042708\n",
      "0.12395203652334372\n",
      "0.12394054789516816\n",
      "0.12392890259913959\n",
      "0.12391708598281552\n",
      "0.1239051131201522\n",
      "0.12389295445122764\n",
      "0.12388064000421388\n",
      "0.12386814030295121\n",
      "0.12385545554189517\n",
      "0.1238426008085795\n",
      "0.1238295466879861\n",
      "0.12381630826267152\n",
      "0.12380288579764556\n",
      "0.12378926460459339\n",
      "0.12377544521054087\n",
      "0.12376142779487184\n",
      "0.12374719782841742\n",
      "0.12373275562591618\n",
      "0.12371810149921257\n",
      "0.12370322095833076\n",
      "0.12368812915659906\n",
      "0.12367282635921563\n",
      "0.12365728350815818\n",
      "0.12364150087538117\n",
      "0.12362547885020465\n",
      "0.12360923263736975\n",
      "0.1235927182241128\n",
      "0.1235759656827763\n",
      "0.12355896063380359\n",
      "0.12354168871133722\n",
      "0.12352416514027349\n",
      "0.12350636098504769\n",
      "0.12348827665718727\n",
      "0.12346992751351067\n",
      "0.12345128447826656\n",
      "0.12343234812340158\n",
      "0.12341313383738191\n",
      "0.1233935978153812\n",
      "0.12337377024480806\n",
      "0.1233536370090699\n",
      "0.12333316921408163\n",
      "0.12331239719868434\n",
      "0.12329127724571452\n",
      "0.1232698249243164\n",
      "0.12324804102581154\n",
      "0.12322589674199996\n",
      "0.12320340782720243\n",
      "0.12318054534115852\n",
      "0.12315732520620158\n",
      "0.12313373324681591\n",
      "0.12310974138527281\n",
      "0.12308536498532163\n",
      "0.12306059054445667\n",
      "0.1230354042446049\n",
      "0.12300982202188969\n",
      "0.12298380079306231\n",
      "0.12295735628287481\n",
      "0.12293047529031029\n",
      "0.12290314404737335\n",
      "0.12287536403936279\n",
      "0.1228471219918618\n",
      "0.12281840446779509\n"
     ]
    }
   ],
   "source": [
    "for iter in range(1000):\n",
    "    print(optim.train(input_indices,output_indices,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_sample = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<linguamind.linalg.Tensor; proxy of <Swig Object of type 'Tensor *' at 0x1078b4e70> >"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn0 = nn.SparseLinearInput(1000,50)\n",
    "syn0.weights.uniform()\n",
    "syn0.weights -= 0.5\n",
    "syn0.weights /= 50\n",
    "\n",
    "syn1 = nn.SparseLinearOutput(50,1000,n_sample)\n",
    "syn1.weights.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = nn.Sequential()\n",
    "mlp.add(syn0)\n",
    "mlp.add(syn1)\n",
    "\n",
    "criterion = nn.MSECriterion(1,n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = la.Tensor((1,10))\n",
    "target.zero()\n",
    "target.set(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_indices = (1,2,4,5)\n",
    "pred = 3\n",
    "neg_samples = list([6,7,8,9,10,11,12,13,14])\n",
    "target_indices = list([pred]) + neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion.forwards(mlp.forward(context_indices,target_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<linguamind.linalg.Tensor; proxy of <Swig Object of type 'Tensor *' at 0x1078b4630> >"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion.backwards(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = la.Tensor((1,50)).uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syn1.updateOutput(input,(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syn1.output.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = nlp.Text(\"hello world\",\"BIIIIOBIIII\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = text.getVocab(\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.getSequence('tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# text = lm.Text(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = lm.Text(\"hello world\",\"BIIIIOBIIII\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = text.getVocab(\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(vocab.size):\n",
    "    print vocab.getTermAtIndex(i).letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.getSequence(\"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NLP Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Types of Component:\n",
    "- word level\n",
    "- sentence level\n",
    "- document level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = lm.Text(\"/my/folder/of/files\") # atempts to autodetect file type\n",
    "c = lm.Text(path=\"myfile.txt\",type=\"raw\")\n",
    "c = lm.Text(path=\"myfile.txt\",type=\"csv-token\") # one token per line in CSV\n",
    "c = lm.Text(path=\"myfile.txt\",type=\"csv-segment\") # text segment per line in CSV\n",
    "c = lm.Text(path=\"myfile.txt\",type=\"prefix__label__\") # one sentence per line with special labels\n",
    "c = lm.Text(path=\"myfile.txt\",type=\"json\")\n",
    "\n",
    "# document level raw text\n",
    "d1 = lm.Text(\"This is my sentence.\")\n",
    "d2 = lm.Text(\"myfile.txt\")\n",
    "\n",
    "c = lm.Text([d1,d2]) #form documents into Corpus\n",
    "\n",
    "# document level text with labels\n",
    "contents = {}\n",
    "contents[\"text\"] = \"This is my sentence. This is another one.\"\n",
    "contents[\"date\"] = 1997\n",
    "d = lm.Text(contents)\n",
    "\n",
    "# sentence level text with labels\n",
    "contents = {}\n",
    "contents[\"text\"] = \"This is my sentence.\"\n",
    "contents[\"sentiment\"] = \"Positive\"\n",
    "contents[\"speaker\"] = \"John Locke\"\n",
    "d = lm.Text(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok = lm.Segmenter(name=\"tokenizer\",lang=\"en\")\n",
    "eos = lm.Segmenter(name=\"eos\",lang=\"en\")\n",
    "\n",
    "# document level text with labels\n",
    "contents = {}\n",
    "contents[\"text\"] = \"This is my sentence. This is another one.\"\n",
    "contents[\"date\"] = 1997\n",
    "d = lm.Text(contents)\n",
    "\n",
    "tok.segment(d)\n",
    "eos.segment(d)\n",
    "\n",
    "print d[\"text\"] # \"This is my sentence. This is another one.\"\n",
    "print d[\"tokens\"] # [\"This\", \"is\"...\n",
    "print d[\"sentences\"] # [[\"This\", \"is\", \"my\", \"sentence\",\".\"],[\"This\",\"is\",\"another\",\"one\",\".\"]]                \n",
    "                     \n",
    "pos = lm.Classifier(name=\"pos\")\n",
    "pos.predict(d)\n",
    "\n",
    "print d[\"sentences\"] # [[{\"token\":\"This\",\"POS\":\"NNP\"},...                \n",
    "\n",
    "sentiment = lm.Classifier(name=\"sentiment\")\n",
    "sentiment.predict(d)\n",
    "print d[\"sentences\"] # [{\"sentiment\":\"Positive\",\"tokens\":[{\"token\":\"This\",\"POS\":\"NNP\"},..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Creating Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a Segmenter (tokenizer)\n",
    "contents = {}\n",
    "contents[\"text\"] = \"I like pie.\"\n",
    "contents[\"char_segs\"] = \"BOBIIIOBIIB\"\n",
    "d = lm.Text(contents)\n",
    "\n",
    "feats=[[\"text_-2\",\"text_-1\"],\"text_-1\",\"text_0\",\"text_1\",[\"text_1\",\"text_2\"]]\n",
    "\n",
    "ambiguity_hash = lm.AmbiguityHash(d,feats,\"char_segs\",threshold=0.95,min_count=5)\n",
    "\n",
    "percept_model = lm.ml.Sequential()\n",
    "percept_model.add(lm.ml.Linear(size=[d.getVocab(\"text\").size() * 10],encoding=\"hashtable\"))\n",
    "percept_model.add(lm.ml.LogSoftmax())\n",
    "\n",
    "percept_loss = lm.ml.PerceptronLoss()\n",
    "percept_optimizer = lm.ml.optim.PerceptronUpdate()\n",
    "percept_searcher = lm.ml.search.BeamSearch(beam=5)\n",
    "\n",
    "# where available in leau of calling all the forward/backprop logic from Python slowing things down\n",
    "tokenizer = lm.ml.trainer.FastSegmentationTrainer(d,\"char_segs\",feats,percept_model,percept_loss,percept_searcher,inherits=[ambiguity_hash],threads=50)\n",
    "\n",
    "tokenizer.segment(d,\"tokens\") #generates new vocab for what was segmented\n",
    "\n",
    "eos = lm.pretrained.Segmenter(name=\"eos\",lang=\"en\")\n",
    "eos.segment(d,\"sentences\",ignore_vocab=True) # does not generate a new vocab for what was segmented\n",
    "\n",
    "# https://github.com/oxford-cs-ml-2015/practical6/blob/master/train.lua\n",
    "lstm_hidden = 50\n",
    "seq_length = 16\n",
    "\n",
    "layers = list()\n",
    "for i in range(seq_length):\n",
    "    layer = {}\n",
    "    \n",
    "    inputs = {}\n",
    "    layer['embed'] = lm.ml.Embedding(d.getVocab(\"token\").size(),lstm_hidden)\n",
    "    layer['lstm'] = lm.ml.LSTM(lstm_hidden,input_x=layer['embed'],input_h=layers[i-1]['lstm']['h'])\n",
    "    layer['softmax'] = lm.ml.LogElasticHierarchicalSoftmax(input=layer['lstm']['h'],size=(lstm_hidden,d.getVocab(\"token\").size()),sample_rate=0.001)\n",
    "    layer['criterion'] = lm.ml.LogElasticHierarchicalSoftmaxLoss()\n",
    "    layers.append(layer)\n",
    "\n",
    "word_language_model_searcher = lm.ml.search.BeamSearch(beam=5)\n",
    "percept_optimizer = lm.ml.optim.SGD()\n",
    "word_language_model = lm.ml.trainer.FastLanguageModelTrainer(d,layers,word_language_model_searcher,percept_optimizer,predict_on=\"token\",bound_on=\"sentence\",threads=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ML Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ambiguity Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Neural Index"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
