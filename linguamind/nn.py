# This file was automatically generated by SWIG (http://www.swig.org).
# Version 3.0.11
#
# Do not make changes to this file unless you know what you are doing--modify
# the SWIG interface file instead.

from sys import version_info as _swig_python_version_info
if _swig_python_version_info >= (2, 7, 0):
    def swig_import_helper():
        import importlib
        pkg = __name__.rpartition('.')[0]
        mname = '.'.join((pkg, '_nn')).lstrip('.')
        try:
            return importlib.import_module(mname)
        except ImportError:
            return importlib.import_module('_nn')
    _nn = swig_import_helper()
    del swig_import_helper
elif _swig_python_version_info >= (2, 6, 0):
    def swig_import_helper():
        from os.path import dirname
        import imp
        fp = None
        try:
            fp, pathname, description = imp.find_module('_nn', [dirname(__file__)])
        except ImportError:
            import _nn
            return _nn
        try:
            _mod = imp.load_module('_nn', fp, pathname, description)
        finally:
            if fp is not None:
                fp.close()
        return _mod
    _nn = swig_import_helper()
    del swig_import_helper
else:
    import _nn
del _swig_python_version_info

try:
    _swig_property = property
except NameError:
    pass  # Python < 2.2 doesn't have 'property'.

try:
    import builtins as __builtin__
except ImportError:
    import __builtin__

def _swig_setattr_nondynamic(self, class_type, name, value, static=1):
    if (name == "thisown"):
        return self.this.own(value)
    if (name == "this"):
        if type(value).__name__ == 'SwigPyObject':
            self.__dict__[name] = value
            return
    method = class_type.__swig_setmethods__.get(name, None)
    if method:
        return method(self, value)
    if (not static):
        if _newclass:
            object.__setattr__(self, name, value)
        else:
            self.__dict__[name] = value
    else:
        raise AttributeError("You cannot add attributes to %s" % self)


def _swig_setattr(self, class_type, name, value):
    return _swig_setattr_nondynamic(self, class_type, name, value, 0)


def _swig_getattr(self, class_type, name):
    if (name == "thisown"):
        return self.this.own()
    method = class_type.__swig_getmethods__.get(name, None)
    if method:
        return method(self)
    raise AttributeError("'%s' object has no attribute '%s'" % (class_type.__name__, name))


def _swig_repr(self):
    try:
        strthis = "proxy of " + self.this.__repr__()
    except __builtin__.Exception:
        strthis = ""
    return "<%s.%s; %s >" % (self.__class__.__module__, self.__class__.__name__, strthis,)

try:
    _object = object
    _newclass = 1
except __builtin__.Exception:
    class _object:
        pass
    _newclass = 0

try:
    import weakref
    weakref_proxy = weakref.proxy
except __builtin__.Exception:
    weakref_proxy = lambda x: x


class SwigPyIterator(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, SwigPyIterator, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, SwigPyIterator, name)

    def __init__(self, *args, **kwargs):
        raise AttributeError("No constructor defined - class is abstract")
    __repr__ = _swig_repr
    __swig_destroy__ = _nn.delete_SwigPyIterator
    __del__ = lambda self: None

    def value(self) -> "PyObject *":
        return _nn.SwigPyIterator_value(self)

    def incr(self, n: 'size_t'=1) -> "swig::SwigPyIterator *":
        return _nn.SwigPyIterator_incr(self, n)

    def decr(self, n: 'size_t'=1) -> "swig::SwigPyIterator *":
        return _nn.SwigPyIterator_decr(self, n)

    def distance(self, x: 'SwigPyIterator') -> "ptrdiff_t":
        return _nn.SwigPyIterator_distance(self, x)

    def equal(self, x: 'SwigPyIterator') -> "bool":
        return _nn.SwigPyIterator_equal(self, x)

    def copy(self) -> "swig::SwigPyIterator *":
        return _nn.SwigPyIterator_copy(self)

    def next(self) -> "PyObject *":
        return _nn.SwigPyIterator_next(self)

    def __next__(self) -> "PyObject *":
        return _nn.SwigPyIterator___next__(self)

    def previous(self) -> "PyObject *":
        return _nn.SwigPyIterator_previous(self)

    def advance(self, n: 'ptrdiff_t') -> "swig::SwigPyIterator *":
        return _nn.SwigPyIterator_advance(self, n)

    def __eq__(self, x: 'SwigPyIterator') -> "bool":
        return _nn.SwigPyIterator___eq__(self, x)

    def __ne__(self, x: 'SwigPyIterator') -> "bool":
        return _nn.SwigPyIterator___ne__(self, x)

    def __iadd__(self, n: 'ptrdiff_t') -> "swig::SwigPyIterator &":
        return _nn.SwigPyIterator___iadd__(self, n)

    def __isub__(self, n: 'ptrdiff_t') -> "swig::SwigPyIterator &":
        return _nn.SwigPyIterator___isub__(self, n)

    def __add__(self, n: 'ptrdiff_t') -> "swig::SwigPyIterator *":
        return _nn.SwigPyIterator___add__(self, n)

    def __sub__(self, *args) -> "ptrdiff_t":
        return _nn.SwigPyIterator___sub__(self, *args)
    def __iter__(self):
        return self
SwigPyIterator_swigregister = _nn.SwigPyIterator_swigregister
SwigPyIterator_swigregister(SwigPyIterator)

class vectori(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, vectori, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, vectori, name)
    __repr__ = _swig_repr

    def iterator(self) -> "swig::SwigPyIterator *":
        return _nn.vectori_iterator(self)
    def __iter__(self):
        return self.iterator()

    def __nonzero__(self) -> "bool":
        return _nn.vectori___nonzero__(self)

    def __bool__(self) -> "bool":
        return _nn.vectori___bool__(self)

    def __len__(self) -> "std::vector< int >::size_type":
        return _nn.vectori___len__(self)

    def __getslice__(self, i: 'std::vector< int >::difference_type', j: 'std::vector< int >::difference_type') -> "std::vector< int,std::allocator< int > > *":
        return _nn.vectori___getslice__(self, i, j)

    def __setslice__(self, *args) -> "void":
        return _nn.vectori___setslice__(self, *args)

    def __delslice__(self, i: 'std::vector< int >::difference_type', j: 'std::vector< int >::difference_type') -> "void":
        return _nn.vectori___delslice__(self, i, j)

    def __delitem__(self, *args) -> "void":
        return _nn.vectori___delitem__(self, *args)

    def __getitem__(self, *args) -> "std::vector< int >::value_type const &":
        return _nn.vectori___getitem__(self, *args)

    def __setitem__(self, *args) -> "void":
        return _nn.vectori___setitem__(self, *args)

    def pop(self) -> "std::vector< int >::value_type":
        return _nn.vectori_pop(self)

    def append(self, x: 'std::vector< int >::value_type const &') -> "void":
        return _nn.vectori_append(self, x)

    def empty(self) -> "bool":
        return _nn.vectori_empty(self)

    def size(self) -> "std::vector< int >::size_type":
        return _nn.vectori_size(self)

    def swap(self, v: 'vectori') -> "void":
        return _nn.vectori_swap(self, v)

    def begin(self) -> "std::vector< int >::iterator":
        return _nn.vectori_begin(self)

    def end(self) -> "std::vector< int >::iterator":
        return _nn.vectori_end(self)

    def rbegin(self) -> "std::vector< int >::reverse_iterator":
        return _nn.vectori_rbegin(self)

    def rend(self) -> "std::vector< int >::reverse_iterator":
        return _nn.vectori_rend(self)

    def clear(self) -> "void":
        return _nn.vectori_clear(self)

    def get_allocator(self) -> "std::vector< int >::allocator_type":
        return _nn.vectori_get_allocator(self)

    def pop_back(self) -> "void":
        return _nn.vectori_pop_back(self)

    def erase(self, *args) -> "std::vector< int >::iterator":
        return _nn.vectori_erase(self, *args)

    def __init__(self, *args):
        this = _nn.new_vectori(*args)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def push_back(self, x: 'std::vector< int >::value_type const &') -> "void":
        return _nn.vectori_push_back(self, x)

    def front(self) -> "std::vector< int >::value_type const &":
        return _nn.vectori_front(self)

    def back(self) -> "std::vector< int >::value_type const &":
        return _nn.vectori_back(self)

    def assign(self, n: 'std::vector< int >::size_type', x: 'std::vector< int >::value_type const &') -> "void":
        return _nn.vectori_assign(self, n, x)

    def resize(self, *args) -> "void":
        return _nn.vectori_resize(self, *args)

    def insert(self, *args) -> "void":
        return _nn.vectori_insert(self, *args)

    def reserve(self, n: 'std::vector< int >::size_type') -> "void":
        return _nn.vectori_reserve(self, n)

    def capacity(self) -> "std::vector< int >::size_type":
        return _nn.vectori_capacity(self)
    __swig_destroy__ = _nn.delete_vectori
    __del__ = lambda self: None
vectori_swigregister = _nn.vectori_swigregister
vectori_swigregister(vectori)

class vectord(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, vectord, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, vectord, name)
    __repr__ = _swig_repr

    def iterator(self) -> "swig::SwigPyIterator *":
        return _nn.vectord_iterator(self)
    def __iter__(self):
        return self.iterator()

    def __nonzero__(self) -> "bool":
        return _nn.vectord___nonzero__(self)

    def __bool__(self) -> "bool":
        return _nn.vectord___bool__(self)

    def __len__(self) -> "std::vector< double >::size_type":
        return _nn.vectord___len__(self)

    def __getslice__(self, i: 'std::vector< double >::difference_type', j: 'std::vector< double >::difference_type') -> "std::vector< double,std::allocator< double > > *":
        return _nn.vectord___getslice__(self, i, j)

    def __setslice__(self, *args) -> "void":
        return _nn.vectord___setslice__(self, *args)

    def __delslice__(self, i: 'std::vector< double >::difference_type', j: 'std::vector< double >::difference_type') -> "void":
        return _nn.vectord___delslice__(self, i, j)

    def __delitem__(self, *args) -> "void":
        return _nn.vectord___delitem__(self, *args)

    def __getitem__(self, *args) -> "std::vector< double >::value_type const &":
        return _nn.vectord___getitem__(self, *args)

    def __setitem__(self, *args) -> "void":
        return _nn.vectord___setitem__(self, *args)

    def pop(self) -> "std::vector< double >::value_type":
        return _nn.vectord_pop(self)

    def append(self, x: 'std::vector< double >::value_type const &') -> "void":
        return _nn.vectord_append(self, x)

    def empty(self) -> "bool":
        return _nn.vectord_empty(self)

    def size(self) -> "std::vector< double >::size_type":
        return _nn.vectord_size(self)

    def swap(self, v: 'vectord') -> "void":
        return _nn.vectord_swap(self, v)

    def begin(self) -> "std::vector< double >::iterator":
        return _nn.vectord_begin(self)

    def end(self) -> "std::vector< double >::iterator":
        return _nn.vectord_end(self)

    def rbegin(self) -> "std::vector< double >::reverse_iterator":
        return _nn.vectord_rbegin(self)

    def rend(self) -> "std::vector< double >::reverse_iterator":
        return _nn.vectord_rend(self)

    def clear(self) -> "void":
        return _nn.vectord_clear(self)

    def get_allocator(self) -> "std::vector< double >::allocator_type":
        return _nn.vectord_get_allocator(self)

    def pop_back(self) -> "void":
        return _nn.vectord_pop_back(self)

    def erase(self, *args) -> "std::vector< double >::iterator":
        return _nn.vectord_erase(self, *args)

    def __init__(self, *args):
        this = _nn.new_vectord(*args)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def push_back(self, x: 'std::vector< double >::value_type const &') -> "void":
        return _nn.vectord_push_back(self, x)

    def front(self) -> "std::vector< double >::value_type const &":
        return _nn.vectord_front(self)

    def back(self) -> "std::vector< double >::value_type const &":
        return _nn.vectord_back(self)

    def assign(self, n: 'std::vector< double >::size_type', x: 'std::vector< double >::value_type const &') -> "void":
        return _nn.vectord_assign(self, n, x)

    def resize(self, *args) -> "void":
        return _nn.vectord_resize(self, *args)

    def insert(self, *args) -> "void":
        return _nn.vectord_insert(self, *args)

    def reserve(self, n: 'std::vector< double >::size_type') -> "void":
        return _nn.vectord_reserve(self, n)

    def capacity(self) -> "std::vector< double >::size_type":
        return _nn.vectord_capacity(self)
    __swig_destroy__ = _nn.delete_vectord
    __del__ = lambda self: None
vectord_swigregister = _nn.vectord_swigregister
vectord_swigregister(vectord)

class charp(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, charp, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, charp, name)
    __repr__ = _swig_repr

    def __init__(self):
        this = _nn.new_charp()
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this
    __swig_destroy__ = _nn.delete_charp
    __del__ = lambda self: None

    def assign(self, value: 'char') -> "void":
        return _nn.charp_assign(self, value)

    def value(self) -> "char":
        return _nn.charp_value(self)

    def cast(self) -> "char *":
        return _nn.charp_cast(self)
    if _newclass:
        frompointer = staticmethod(_nn.charp_frompointer)
    else:
        frompointer = _nn.charp_frompointer
charp_swigregister = _nn.charp_swigregister
charp_swigregister(charp)

def charp_frompointer(t: 'char *') -> "charp *":
    return _nn.charp_frompointer(t)
charp_frompointer = _nn.charp_frompointer

class Layer(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, Layer, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, Layer, name)

    def __init__(self, *args, **kwargs):
        raise AttributeError("No constructor defined - class is abstract")
    __repr__ = _swig_repr
    __swig_destroy__ = _nn.delete_Layer
    __del__ = lambda self: None

    def duplicateWithSameWeights(self) -> "Layer *":
        return _nn.Layer_duplicateWithSameWeights(self)

    def getInputDim(self) -> "int":
        return _nn.Layer_getInputDim(self)

    def getOutputDim(self) -> "int":
        return _nn.Layer_getOutputDim(self)

    def hasSparseInput(self) -> "bool":
        return _nn.Layer_hasSparseInput(self)

    def hasSparseOutput(self) -> "bool":
        return _nn.Layer_hasSparseOutput(self)

    def getOutput(self) -> "Vector *":
        return _nn.Layer_getOutput(self)

    def getInputGrad(self) -> "Vector *":
        return _nn.Layer_getInputGrad(self)

    def getFullOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.Layer_getFullOutputIndices(self)

    def updateOutput(self, arg2: 'Vector *', sparse_output: 'vectori') -> "int":
        return _nn.Layer_updateOutput(self, arg2, sparse_output)

    def updateInputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.Layer_updateInputGrad(self, output_grad)

    def accGradParameters(self, input: 'Vector *', output_grad: 'Vector *', alpha: 'float') -> "int":
        return _nn.Layer_accGradParameters(self, input, output_grad, alpha)
Layer_swigregister = _nn.Layer_swigregister
Layer_swigregister(Layer)

class FlexLayer(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, FlexLayer, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, FlexLayer, name)

    def __init__(self, *args, **kwargs):
        raise AttributeError("No constructor defined - class is abstract")
    __repr__ = _swig_repr
    __swig_destroy__ = _nn.delete_FlexLayer
    __del__ = lambda self: None

    def duplicateWithSameWeights(self) -> "FlexLayer *":
        return _nn.FlexLayer_duplicateWithSameWeights(self)

    def getInputDim(self) -> "int":
        return _nn.FlexLayer_getInputDim(self)

    def getOutputDim(self) -> "int":
        return _nn.FlexLayer_getOutputDim(self)

    def inputMustBeSparse(self) -> "bool":
        return _nn.FlexLayer_inputMustBeSparse(self)

    def outputMustBeSparse(self) -> "bool":
        return _nn.FlexLayer_outputMustBeSparse(self)

    def mandatoryIdenticalInputOutputSparsity(self) -> "bool":
        return _nn.FlexLayer_mandatoryIdenticalInputOutputSparsity(self)

    def containsLayers(self) -> "bool":
        return _nn.FlexLayer_containsLayers(self)

    def getOutput(self) -> "Vector *":
        return _nn.FlexLayer_getOutput(self)

    def getOutputIndices(self) -> "std::vector< int,std::allocator< int > > &":
        return _nn.FlexLayer_getOutputIndices(self)

    def getInputGrad(self) -> "Vector *":
        return _nn.FlexLayer_getInputGrad(self)

    def setOutputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.FlexLayer_setOutputGrad(self, output_grad)

    def getFullOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.FlexLayer_getFullOutputIndices(self)

    def updateOutputDenseToDense(self, input: 'Vector *') -> "int":
        return _nn.FlexLayer_updateOutputDenseToDense(self, input)

    def updateOutputDenseToWeightedSparse(self, input: 'Vector *', sparse_output: 'vectori') -> "int":
        return _nn.FlexLayer_updateOutputDenseToWeightedSparse(self, input, sparse_output)

    def updateOutputWeightedSparseToDense(self, input: 'Vector *', sparse_input: 'vectori') -> "int":
        return _nn.FlexLayer_updateOutputWeightedSparseToDense(self, input, sparse_input)

    def updateOutputWeightedSparseToWeightedSparse(self, input: 'Vector *', sparse_input: 'vectori', sparse_output: 'vectori') -> "int":
        return _nn.FlexLayer_updateOutputWeightedSparseToWeightedSparse(self, input, sparse_input, sparse_output)

    def updateOutputBinarySparseToDense(self, sparse_input: 'vectori') -> "int":
        return _nn.FlexLayer_updateOutputBinarySparseToDense(self, sparse_input)

    def updateOutputBinarySparseToWeightedSparse(self, sparse_input: 'vectori', sparse_output: 'vectori') -> "int":
        return _nn.FlexLayer_updateOutputBinarySparseToWeightedSparse(self, sparse_input, sparse_output)

    def backward(self, output_grad: 'Vector *') -> "int":
        return _nn.FlexLayer_backward(self, output_grad)

    def updateInputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.FlexLayer_updateInputGrad(self, output_grad)

    def accGradParameters(self, alpha: 'float') -> "int":
        return _nn.FlexLayer_accGradParameters(self, alpha)
FlexLayer_swigregister = _nn.FlexLayer_swigregister
FlexLayer_swigregister(FlexLayer)

class LayerBuilder(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, LayerBuilder, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, LayerBuilder, name)
    __repr__ = _swig_repr

    def iterator(self) -> "swig::SwigPyIterator *":
        return _nn.LayerBuilder_iterator(self)
    def __iter__(self):
        return self.iterator()

    def __nonzero__(self) -> "bool":
        return _nn.LayerBuilder___nonzero__(self)

    def __bool__(self) -> "bool":
        return _nn.LayerBuilder___bool__(self)

    def __len__(self) -> "std::vector< Layer * >::size_type":
        return _nn.LayerBuilder___len__(self)

    def __getslice__(self, i: 'std::vector< Layer * >::difference_type', j: 'std::vector< Layer * >::difference_type') -> "std::vector< Layer *,std::allocator< Layer * > > *":
        return _nn.LayerBuilder___getslice__(self, i, j)

    def __setslice__(self, *args) -> "void":
        return _nn.LayerBuilder___setslice__(self, *args)

    def __delslice__(self, i: 'std::vector< Layer * >::difference_type', j: 'std::vector< Layer * >::difference_type') -> "void":
        return _nn.LayerBuilder___delslice__(self, i, j)

    def __delitem__(self, *args) -> "void":
        return _nn.LayerBuilder___delitem__(self, *args)

    def __getitem__(self, *args) -> "std::vector< Layer * >::value_type":
        return _nn.LayerBuilder___getitem__(self, *args)

    def __setitem__(self, *args) -> "void":
        return _nn.LayerBuilder___setitem__(self, *args)

    def pop(self) -> "std::vector< Layer * >::value_type":
        return _nn.LayerBuilder_pop(self)

    def append(self, x: 'Layer') -> "void":
        return _nn.LayerBuilder_append(self, x)

    def empty(self) -> "bool":
        return _nn.LayerBuilder_empty(self)

    def size(self) -> "std::vector< Layer * >::size_type":
        return _nn.LayerBuilder_size(self)

    def swap(self, v: 'LayerBuilder') -> "void":
        return _nn.LayerBuilder_swap(self, v)

    def begin(self) -> "std::vector< Layer * >::iterator":
        return _nn.LayerBuilder_begin(self)

    def end(self) -> "std::vector< Layer * >::iterator":
        return _nn.LayerBuilder_end(self)

    def rbegin(self) -> "std::vector< Layer * >::reverse_iterator":
        return _nn.LayerBuilder_rbegin(self)

    def rend(self) -> "std::vector< Layer * >::reverse_iterator":
        return _nn.LayerBuilder_rend(self)

    def clear(self) -> "void":
        return _nn.LayerBuilder_clear(self)

    def get_allocator(self) -> "std::vector< Layer * >::allocator_type":
        return _nn.LayerBuilder_get_allocator(self)

    def pop_back(self) -> "void":
        return _nn.LayerBuilder_pop_back(self)

    def erase(self, *args) -> "std::vector< Layer * >::iterator":
        return _nn.LayerBuilder_erase(self, *args)

    def __init__(self, *args):
        this = _nn.new_LayerBuilder(*args)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def push_back(self, x: 'Layer') -> "void":
        return _nn.LayerBuilder_push_back(self, x)

    def front(self) -> "std::vector< Layer * >::value_type":
        return _nn.LayerBuilder_front(self)

    def back(self) -> "std::vector< Layer * >::value_type":
        return _nn.LayerBuilder_back(self)

    def assign(self, n: 'std::vector< Layer * >::size_type', x: 'Layer') -> "void":
        return _nn.LayerBuilder_assign(self, n, x)

    def resize(self, *args) -> "void":
        return _nn.LayerBuilder_resize(self, *args)

    def insert(self, *args) -> "void":
        return _nn.LayerBuilder_insert(self, *args)

    def reserve(self, n: 'std::vector< Layer * >::size_type') -> "void":
        return _nn.LayerBuilder_reserve(self, n)

    def capacity(self) -> "std::vector< Layer * >::size_type":
        return _nn.LayerBuilder_capacity(self)
    __swig_destroy__ = _nn.delete_LayerBuilder
    __del__ = lambda self: None
LayerBuilder_swigregister = _nn.LayerBuilder_swigregister
LayerBuilder_swigregister(LayerBuilder)

class FlexLayerBuilder(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, FlexLayerBuilder, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, FlexLayerBuilder, name)
    __repr__ = _swig_repr

    def iterator(self) -> "swig::SwigPyIterator *":
        return _nn.FlexLayerBuilder_iterator(self)
    def __iter__(self):
        return self.iterator()

    def __nonzero__(self) -> "bool":
        return _nn.FlexLayerBuilder___nonzero__(self)

    def __bool__(self) -> "bool":
        return _nn.FlexLayerBuilder___bool__(self)

    def __len__(self) -> "std::vector< FlexLayer * >::size_type":
        return _nn.FlexLayerBuilder___len__(self)

    def __getslice__(self, i: 'std::vector< FlexLayer * >::difference_type', j: 'std::vector< FlexLayer * >::difference_type') -> "std::vector< FlexLayer *,std::allocator< FlexLayer * > > *":
        return _nn.FlexLayerBuilder___getslice__(self, i, j)

    def __setslice__(self, *args) -> "void":
        return _nn.FlexLayerBuilder___setslice__(self, *args)

    def __delslice__(self, i: 'std::vector< FlexLayer * >::difference_type', j: 'std::vector< FlexLayer * >::difference_type') -> "void":
        return _nn.FlexLayerBuilder___delslice__(self, i, j)

    def __delitem__(self, *args) -> "void":
        return _nn.FlexLayerBuilder___delitem__(self, *args)

    def __getitem__(self, *args) -> "std::vector< FlexLayer * >::value_type":
        return _nn.FlexLayerBuilder___getitem__(self, *args)

    def __setitem__(self, *args) -> "void":
        return _nn.FlexLayerBuilder___setitem__(self, *args)

    def pop(self) -> "std::vector< FlexLayer * >::value_type":
        return _nn.FlexLayerBuilder_pop(self)

    def append(self, x: 'FlexLayer') -> "void":
        return _nn.FlexLayerBuilder_append(self, x)

    def empty(self) -> "bool":
        return _nn.FlexLayerBuilder_empty(self)

    def size(self) -> "std::vector< FlexLayer * >::size_type":
        return _nn.FlexLayerBuilder_size(self)

    def swap(self, v: 'FlexLayerBuilder') -> "void":
        return _nn.FlexLayerBuilder_swap(self, v)

    def begin(self) -> "std::vector< FlexLayer * >::iterator":
        return _nn.FlexLayerBuilder_begin(self)

    def end(self) -> "std::vector< FlexLayer * >::iterator":
        return _nn.FlexLayerBuilder_end(self)

    def rbegin(self) -> "std::vector< FlexLayer * >::reverse_iterator":
        return _nn.FlexLayerBuilder_rbegin(self)

    def rend(self) -> "std::vector< FlexLayer * >::reverse_iterator":
        return _nn.FlexLayerBuilder_rend(self)

    def clear(self) -> "void":
        return _nn.FlexLayerBuilder_clear(self)

    def get_allocator(self) -> "std::vector< FlexLayer * >::allocator_type":
        return _nn.FlexLayerBuilder_get_allocator(self)

    def pop_back(self) -> "void":
        return _nn.FlexLayerBuilder_pop_back(self)

    def erase(self, *args) -> "std::vector< FlexLayer * >::iterator":
        return _nn.FlexLayerBuilder_erase(self, *args)

    def __init__(self, *args):
        this = _nn.new_FlexLayerBuilder(*args)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def push_back(self, x: 'FlexLayer') -> "void":
        return _nn.FlexLayerBuilder_push_back(self, x)

    def front(self) -> "std::vector< FlexLayer * >::value_type":
        return _nn.FlexLayerBuilder_front(self)

    def back(self) -> "std::vector< FlexLayer * >::value_type":
        return _nn.FlexLayerBuilder_back(self)

    def assign(self, n: 'std::vector< FlexLayer * >::size_type', x: 'FlexLayer') -> "void":
        return _nn.FlexLayerBuilder_assign(self, n, x)

    def resize(self, *args) -> "void":
        return _nn.FlexLayerBuilder_resize(self, *args)

    def insert(self, *args) -> "void":
        return _nn.FlexLayerBuilder_insert(self, *args)

    def reserve(self, n: 'std::vector< FlexLayer * >::size_type') -> "void":
        return _nn.FlexLayerBuilder_reserve(self, n)

    def capacity(self) -> "std::vector< FlexLayer * >::size_type":
        return _nn.FlexLayerBuilder_capacity(self)
    __swig_destroy__ = _nn.delete_FlexLayerBuilder
    __del__ = lambda self: None
FlexLayerBuilder_swigregister = _nn.FlexLayerBuilder_swigregister
FlexLayerBuilder_swigregister(FlexLayerBuilder)

class TrainingExample(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, TrainingExample, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, TrainingExample, name)
    __repr__ = _swig_repr

    def iterator(self) -> "swig::SwigPyIterator *":
        return _nn.TrainingExample_iterator(self)
    def __iter__(self):
        return self.iterator()

    def __nonzero__(self) -> "bool":
        return _nn.TrainingExample___nonzero__(self)

    def __bool__(self) -> "bool":
        return _nn.TrainingExample___bool__(self)

    def __len__(self) -> "std::vector< std::vector< int > >::size_type":
        return _nn.TrainingExample___len__(self)

    def __getslice__(self, i: 'std::vector< std::vector< int > >::difference_type', j: 'std::vector< std::vector< int > >::difference_type') -> "std::vector< std::vector< int,std::allocator< int > >,std::allocator< std::vector< int,std::allocator< int > > > > *":
        return _nn.TrainingExample___getslice__(self, i, j)

    def __setslice__(self, *args) -> "void":
        return _nn.TrainingExample___setslice__(self, *args)

    def __delslice__(self, i: 'std::vector< std::vector< int > >::difference_type', j: 'std::vector< std::vector< int > >::difference_type') -> "void":
        return _nn.TrainingExample___delslice__(self, i, j)

    def __delitem__(self, *args) -> "void":
        return _nn.TrainingExample___delitem__(self, *args)

    def __getitem__(self, *args) -> "std::vector< std::vector< int > >::value_type const &":
        return _nn.TrainingExample___getitem__(self, *args)

    def __setitem__(self, *args) -> "void":
        return _nn.TrainingExample___setitem__(self, *args)

    def pop(self) -> "std::vector< std::vector< int > >::value_type":
        return _nn.TrainingExample_pop(self)

    def append(self, x: 'vectori') -> "void":
        return _nn.TrainingExample_append(self, x)

    def empty(self) -> "bool":
        return _nn.TrainingExample_empty(self)

    def size(self) -> "std::vector< std::vector< int > >::size_type":
        return _nn.TrainingExample_size(self)

    def swap(self, v: 'TrainingExample') -> "void":
        return _nn.TrainingExample_swap(self, v)

    def begin(self) -> "std::vector< std::vector< int > >::iterator":
        return _nn.TrainingExample_begin(self)

    def end(self) -> "std::vector< std::vector< int > >::iterator":
        return _nn.TrainingExample_end(self)

    def rbegin(self) -> "std::vector< std::vector< int > >::reverse_iterator":
        return _nn.TrainingExample_rbegin(self)

    def rend(self) -> "std::vector< std::vector< int > >::reverse_iterator":
        return _nn.TrainingExample_rend(self)

    def clear(self) -> "void":
        return _nn.TrainingExample_clear(self)

    def get_allocator(self) -> "std::vector< std::vector< int > >::allocator_type":
        return _nn.TrainingExample_get_allocator(self)

    def pop_back(self) -> "void":
        return _nn.TrainingExample_pop_back(self)

    def erase(self, *args) -> "std::vector< std::vector< int > >::iterator":
        return _nn.TrainingExample_erase(self, *args)

    def __init__(self, *args):
        this = _nn.new_TrainingExample(*args)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def push_back(self, x: 'vectori') -> "void":
        return _nn.TrainingExample_push_back(self, x)

    def front(self) -> "std::vector< std::vector< int > >::value_type const &":
        return _nn.TrainingExample_front(self)

    def back(self) -> "std::vector< std::vector< int > >::value_type const &":
        return _nn.TrainingExample_back(self)

    def assign(self, n: 'std::vector< std::vector< int > >::size_type', x: 'vectori') -> "void":
        return _nn.TrainingExample_assign(self, n, x)

    def resize(self, *args) -> "void":
        return _nn.TrainingExample_resize(self, *args)

    def insert(self, *args) -> "void":
        return _nn.TrainingExample_insert(self, *args)

    def reserve(self, n: 'std::vector< std::vector< int > >::size_type') -> "void":
        return _nn.TrainingExample_reserve(self, n)

    def capacity(self) -> "std::vector< std::vector< int > >::size_type":
        return _nn.TrainingExample_capacity(self)
    __swig_destroy__ = _nn.delete_TrainingExample
    __del__ = lambda self: None
TrainingExample_swigregister = _nn.TrainingExample_swigregister
TrainingExample_swigregister(TrainingExample)

class FlexLinear(FlexLayer):
    __swig_setmethods__ = {}
    for _s in [FlexLayer]:
        __swig_setmethods__.update(getattr(_s, '__swig_setmethods__', {}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, FlexLinear, name, value)
    __swig_getmethods__ = {}
    for _s in [FlexLayer]:
        __swig_getmethods__.update(getattr(_s, '__swig_getmethods__', {}))
    __getattr__ = lambda self, name: _swig_getattr(self, FlexLinear, name)
    __repr__ = _swig_repr

    def __init__(self, input_dim: 'int', output_dim: 'int'):
        this = _nn.new_FlexLinear(input_dim, output_dim)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def init(self, input_dim: 'int', output_dim: 'int') -> "void":
        return _nn.FlexLinear_init(self, input_dim, output_dim)
    __swig_setmethods__["weights"] = _nn.FlexLinear_weights_set
    __swig_getmethods__["weights"] = _nn.FlexLinear_weights_get
    if _newclass:
        weights = _swig_property(_nn.FlexLinear_weights_get, _nn.FlexLinear_weights_set)

    def duplicateWithSameWeights(self) -> "FlexLayer *":
        return _nn.FlexLinear_duplicateWithSameWeights(self)

    def swapInputOutputSparsity(self) -> "int":
        return _nn.FlexLinear_swapInputOutputSparsity(self)

    def mandatoryIdenticalInputOutputSparsity(self) -> "bool":
        return _nn.FlexLinear_mandatoryIdenticalInputOutputSparsity(self)

    def getInputDim(self) -> "int":
        return _nn.FlexLinear_getInputDim(self)

    def getOutputDim(self) -> "int":
        return _nn.FlexLinear_getOutputDim(self)

    def inputMustBeSparse(self) -> "bool":
        return _nn.FlexLinear_inputMustBeSparse(self)

    def outputMustBeSparse(self) -> "bool":
        return _nn.FlexLinear_outputMustBeSparse(self)

    def containsLayers(self) -> "bool":
        return _nn.FlexLinear_containsLayers(self)

    def getOutput(self) -> "Vector *":
        return _nn.FlexLinear_getOutput(self)

    def getOutputIndices(self) -> "std::vector< int,std::allocator< int > > &":
        return _nn.FlexLinear_getOutputIndices(self)

    def getInputGrad(self) -> "Vector *":
        return _nn.FlexLinear_getInputGrad(self)

    def setOutputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.FlexLinear_setOutputGrad(self, output_grad)

    def getFullOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.FlexLinear_getFullOutputIndices(self)

    def updateOutputDenseToDense(self, input: 'Vector *') -> "int":
        return _nn.FlexLinear_updateOutputDenseToDense(self, input)

    def updateOutputDenseToWeightedSparse(self, input: 'Vector *', output_indices: 'vectori') -> "int":
        return _nn.FlexLinear_updateOutputDenseToWeightedSparse(self, input, output_indices)

    def updateOutputWeightedSparseToDense(self, input: 'Vector *', input_indices: 'vectori') -> "int":
        return _nn.FlexLinear_updateOutputWeightedSparseToDense(self, input, input_indices)

    def updateOutputWeightedSparseToWeightedSparse(self, input: 'Vector *', input_indices: 'vectori', output_indices: 'vectori') -> "int":
        return _nn.FlexLinear_updateOutputWeightedSparseToWeightedSparse(self, input, input_indices, output_indices)

    def updateOutputBinarySparseToDense(self, input_indices: 'vectori') -> "int":
        return _nn.FlexLinear_updateOutputBinarySparseToDense(self, input_indices)

    def updateOutputBinarySparseToWeightedSparse(self, input_indices: 'vectori', output_indices: 'vectori') -> "int":
        return _nn.FlexLinear_updateOutputBinarySparseToWeightedSparse(self, input_indices, output_indices)

    def backward(self, output_grad: 'Vector *') -> "int":
        return _nn.FlexLinear_backward(self, output_grad)

    def updateInputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.FlexLinear_updateInputGrad(self, output_grad)

    def accGradParameters(self, alpha: 'float') -> "int":
        return _nn.FlexLinear_accGradParameters(self, alpha)
    __swig_destroy__ = _nn.delete_FlexLinear
    __del__ = lambda self: None
FlexLinear_swigregister = _nn.FlexLinear_swigregister
FlexLinear_swigregister(FlexLinear)

class SparseLinearInput(Layer):
    __swig_setmethods__ = {}
    for _s in [Layer]:
        __swig_setmethods__.update(getattr(_s, '__swig_setmethods__', {}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, SparseLinearInput, name, value)
    __swig_getmethods__ = {}
    for _s in [Layer]:
        __swig_getmethods__.update(getattr(_s, '__swig_getmethods__', {}))
    __getattr__ = lambda self, name: _swig_getattr(self, SparseLinearInput, name)
    __repr__ = _swig_repr

    def __init__(self, input_dim: 'int', output_dim: 'int'):
        this = _nn.new_SparseLinearInput(input_dim, output_dim)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def init(self, input_dim: 'int', output_dim: 'int') -> "void":
        return _nn.SparseLinearInput_init(self, input_dim, output_dim)
    __swig_setmethods__["weights"] = _nn.SparseLinearInput_weights_set
    __swig_getmethods__["weights"] = _nn.SparseLinearInput_weights_get
    if _newclass:
        weights = _swig_property(_nn.SparseLinearInput_weights_get, _nn.SparseLinearInput_weights_set)

    def duplicateWithSameWeights(self) -> "Layer *":
        return _nn.SparseLinearInput_duplicateWithSameWeights(self)

    def getInputDim(self) -> "int":
        return _nn.SparseLinearInput_getInputDim(self)

    def getOutputDim(self) -> "int":
        return _nn.SparseLinearInput_getOutputDim(self)

    def hasSparseInput(self) -> "bool":
        return _nn.SparseLinearInput_hasSparseInput(self)

    def hasSparseOutput(self) -> "bool":
        return _nn.SparseLinearInput_hasSparseOutput(self)

    def getOutput(self) -> "Vector *":
        return _nn.SparseLinearInput_getOutput(self)

    def getInputGrad(self) -> "Vector *":
        return _nn.SparseLinearInput_getInputGrad(self)

    def getFullOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.SparseLinearInput_getFullOutputIndices(self)

    def updateOutput(self, input: 'Vector *', input_indices: 'vectori') -> "int":
        return _nn.SparseLinearInput_updateOutput(self, input, input_indices)

    def updateInputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.SparseLinearInput_updateInputGrad(self, output_grad)

    def accGradParameters(self, input: 'Vector *', output_grad: 'Vector *', alpha: 'float') -> "int":
        return _nn.SparseLinearInput_accGradParameters(self, input, output_grad, alpha)
    __swig_destroy__ = _nn.delete_SparseLinearInput
    __del__ = lambda self: None
SparseLinearInput_swigregister = _nn.SparseLinearInput_swigregister
SparseLinearInput_swigregister(SparseLinearInput)

class SparseLinearOutput(Layer):
    __swig_setmethods__ = {}
    for _s in [Layer]:
        __swig_setmethods__.update(getattr(_s, '__swig_setmethods__', {}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, SparseLinearOutput, name, value)
    __swig_getmethods__ = {}
    for _s in [Layer]:
        __swig_getmethods__.update(getattr(_s, '__swig_getmethods__', {}))
    __getattr__ = lambda self, name: _swig_getattr(self, SparseLinearOutput, name)
    __repr__ = _swig_repr

    def __init__(self, input_dim: 'int', output_dim: 'int'):
        this = _nn.new_SparseLinearOutput(input_dim, output_dim)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def init(self, input_dim: 'int', output_dim: 'int') -> "void":
        return _nn.SparseLinearOutput_init(self, input_dim, output_dim)
    __swig_setmethods__["weights"] = _nn.SparseLinearOutput_weights_set
    __swig_getmethods__["weights"] = _nn.SparseLinearOutput_weights_get
    if _newclass:
        weights = _swig_property(_nn.SparseLinearOutput_weights_get, _nn.SparseLinearOutput_weights_set)

    def duplicateWithSameWeights(self) -> "Layer *":
        return _nn.SparseLinearOutput_duplicateWithSameWeights(self)

    def getInputDim(self) -> "int":
        return _nn.SparseLinearOutput_getInputDim(self)

    def getOutputDim(self) -> "int":
        return _nn.SparseLinearOutput_getOutputDim(self)

    def hasSparseInput(self) -> "bool":
        return _nn.SparseLinearOutput_hasSparseInput(self)

    def hasSparseOutput(self) -> "bool":
        return _nn.SparseLinearOutput_hasSparseOutput(self)

    def getOutput(self) -> "Vector *":
        return _nn.SparseLinearOutput_getOutput(self)

    def getInputGrad(self) -> "Vector *":
        return _nn.SparseLinearOutput_getInputGrad(self)

    def getFullOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.SparseLinearOutput_getFullOutputIndices(self)

    def updateOutput(self, input: 'Vector *', output_indices: 'vectori') -> "int":
        return _nn.SparseLinearOutput_updateOutput(self, input, output_indices)

    def updateInputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.SparseLinearOutput_updateInputGrad(self, output_grad)

    def accGradParameters(self, input: 'Vector *', output_grad: 'Vector *', alpha: 'float') -> "int":
        return _nn.SparseLinearOutput_accGradParameters(self, input, output_grad, alpha)
    __swig_destroy__ = _nn.delete_SparseLinearOutput
    __del__ = lambda self: None
SparseLinearOutput_swigregister = _nn.SparseLinearOutput_swigregister
SparseLinearOutput_swigregister(SparseLinearOutput)

class WeightedSparseLinearInput(Layer):
    __swig_setmethods__ = {}
    for _s in [Layer]:
        __swig_setmethods__.update(getattr(_s, '__swig_setmethods__', {}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, WeightedSparseLinearInput, name, value)
    __swig_getmethods__ = {}
    for _s in [Layer]:
        __swig_getmethods__.update(getattr(_s, '__swig_getmethods__', {}))
    __getattr__ = lambda self, name: _swig_getattr(self, WeightedSparseLinearInput, name)
    __repr__ = _swig_repr

    def __init__(self, input_dim: 'int', output_dim: 'int'):
        this = _nn.new_WeightedSparseLinearInput(input_dim, output_dim)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def init(self, input_dim: 'int', output_dim: 'int') -> "void":
        return _nn.WeightedSparseLinearInput_init(self, input_dim, output_dim)
    __swig_setmethods__["weights"] = _nn.WeightedSparseLinearInput_weights_set
    __swig_getmethods__["weights"] = _nn.WeightedSparseLinearInput_weights_get
    if _newclass:
        weights = _swig_property(_nn.WeightedSparseLinearInput_weights_get, _nn.WeightedSparseLinearInput_weights_set)

    def duplicateWithSameWeights(self) -> "Layer *":
        return _nn.WeightedSparseLinearInput_duplicateWithSameWeights(self)

    def getInputDim(self) -> "int":
        return _nn.WeightedSparseLinearInput_getInputDim(self)

    def getOutputDim(self) -> "int":
        return _nn.WeightedSparseLinearInput_getOutputDim(self)

    def hasSparseInput(self) -> "bool":
        return _nn.WeightedSparseLinearInput_hasSparseInput(self)

    def hasSparseOutput(self) -> "bool":
        return _nn.WeightedSparseLinearInput_hasSparseOutput(self)

    def getOutput(self) -> "Vector *":
        return _nn.WeightedSparseLinearInput_getOutput(self)

    def getInputGrad(self) -> "Vector *":
        return _nn.WeightedSparseLinearInput_getInputGrad(self)

    def getFullOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.WeightedSparseLinearInput_getFullOutputIndices(self)

    def predict(self, input: 'Vector *', input_indices: 'vectori') -> "int":
        return _nn.WeightedSparseLinearInput_predict(self, input, input_indices)

    def updateOutput(self, input: 'Vector *', input_indices: 'vectori') -> "int":
        return _nn.WeightedSparseLinearInput_updateOutput(self, input, input_indices)

    def updateInputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.WeightedSparseLinearInput_updateInputGrad(self, output_grad)

    def accGradParameters(self, input: 'Vector *', output_grad: 'Vector *', alpha: 'float') -> "int":
        return _nn.WeightedSparseLinearInput_accGradParameters(self, input, output_grad, alpha)
    __swig_destroy__ = _nn.delete_WeightedSparseLinearInput
    __del__ = lambda self: None
WeightedSparseLinearInput_swigregister = _nn.WeightedSparseLinearInput_swigregister
WeightedSparseLinearInput_swigregister(WeightedSparseLinearInput)

class Linear(Layer):
    __swig_setmethods__ = {}
    for _s in [Layer]:
        __swig_setmethods__.update(getattr(_s, '__swig_setmethods__', {}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, Linear, name, value)
    __swig_getmethods__ = {}
    for _s in [Layer]:
        __swig_getmethods__.update(getattr(_s, '__swig_getmethods__', {}))
    __getattr__ = lambda self, name: _swig_getattr(self, Linear, name)
    __repr__ = _swig_repr

    def __init__(self, input_dim: 'int', output_dim: 'int'):
        this = _nn.new_Linear(input_dim, output_dim)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def init(self, input_dim: 'int', output_dim: 'int') -> "void":
        return _nn.Linear_init(self, input_dim, output_dim)
    __swig_setmethods__["weights"] = _nn.Linear_weights_set
    __swig_getmethods__["weights"] = _nn.Linear_weights_get
    if _newclass:
        weights = _swig_property(_nn.Linear_weights_get, _nn.Linear_weights_set)

    def duplicateWithSameWeights(self) -> "Layer *":
        return _nn.Linear_duplicateWithSameWeights(self)

    def getInputDim(self) -> "int":
        return _nn.Linear_getInputDim(self)

    def getOutputDim(self) -> "int":
        return _nn.Linear_getOutputDim(self)

    def hasSparseInput(self) -> "bool":
        return _nn.Linear_hasSparseInput(self)

    def hasSparseOutput(self) -> "bool":
        return _nn.Linear_hasSparseOutput(self)

    def getOutput(self) -> "Vector *":
        return _nn.Linear_getOutput(self)

    def getInputGrad(self) -> "Vector *":
        return _nn.Linear_getInputGrad(self)

    def getFullOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.Linear_getFullOutputIndices(self)

    def updateOutput(self, input: 'Vector *', not_used: 'vectori') -> "int":
        return _nn.Linear_updateOutput(self, input, not_used)

    def updateInputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.Linear_updateInputGrad(self, output_grad)

    def accGradParameters(self, input: 'Vector *', output_grad: 'Vector *', alpha: 'float') -> "int":
        return _nn.Linear_accGradParameters(self, input, output_grad, alpha)
    __swig_destroy__ = _nn.delete_Linear
    __del__ = lambda self: None
Linear_swigregister = _nn.Linear_swigregister
Linear_swigregister(Linear)

class Relu(Layer):
    __swig_setmethods__ = {}
    for _s in [Layer]:
        __swig_setmethods__.update(getattr(_s, '__swig_setmethods__', {}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, Relu, name, value)
    __swig_getmethods__ = {}
    for _s in [Layer]:
        __swig_getmethods__.update(getattr(_s, '__swig_getmethods__', {}))
    __getattr__ = lambda self, name: _swig_getattr(self, Relu, name)
    __repr__ = _swig_repr

    def __init__(self, dim: 'int'):
        this = _nn.new_Relu(dim)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def init(self, dim: 'int') -> "void":
        return _nn.Relu_init(self, dim)

    def duplicateWithSameWeights(self) -> "Layer *":
        return _nn.Relu_duplicateWithSameWeights(self)

    def getInputDim(self) -> "int":
        return _nn.Relu_getInputDim(self)

    def getOutputDim(self) -> "int":
        return _nn.Relu_getOutputDim(self)

    def hasSparseInput(self) -> "bool":
        return _nn.Relu_hasSparseInput(self)

    def hasSparseOutput(self) -> "bool":
        return _nn.Relu_hasSparseOutput(self)

    def getOutput(self) -> "Vector *":
        return _nn.Relu_getOutput(self)

    def getInputGrad(self) -> "Vector *":
        return _nn.Relu_getInputGrad(self)

    def getFullOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.Relu_getFullOutputIndices(self)

    def updateOutput(self, input: 'Vector *', output_indices: 'vectori') -> "int":
        return _nn.Relu_updateOutput(self, input, output_indices)

    def updateInputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.Relu_updateInputGrad(self, output_grad)

    def accGradParameters(self, input: 'Vector *', output_grad: 'Vector *', alpha: 'float') -> "int":
        return _nn.Relu_accGradParameters(self, input, output_grad, alpha)
    __swig_destroy__ = _nn.delete_Relu
    __del__ = lambda self: None
Relu_swigregister = _nn.Relu_swigregister
Relu_swigregister(Relu)

class Sigmoid(Layer):
    __swig_setmethods__ = {}
    for _s in [Layer]:
        __swig_setmethods__.update(getattr(_s, '__swig_setmethods__', {}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, Sigmoid, name, value)
    __swig_getmethods__ = {}
    for _s in [Layer]:
        __swig_getmethods__.update(getattr(_s, '__swig_getmethods__', {}))
    __getattr__ = lambda self, name: _swig_getattr(self, Sigmoid, name)
    __repr__ = _swig_repr

    def __init__(self, dim: 'int'):
        this = _nn.new_Sigmoid(dim)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def init(self, dim: 'int') -> "void":
        return _nn.Sigmoid_init(self, dim)

    def duplicateWithSameWeights(self) -> "Layer *":
        return _nn.Sigmoid_duplicateWithSameWeights(self)

    def getInputDim(self) -> "int":
        return _nn.Sigmoid_getInputDim(self)

    def getOutputDim(self) -> "int":
        return _nn.Sigmoid_getOutputDim(self)

    def hasSparseInput(self) -> "bool":
        return _nn.Sigmoid_hasSparseInput(self)

    def hasSparseOutput(self) -> "bool":
        return _nn.Sigmoid_hasSparseOutput(self)

    def getOutput(self) -> "Vector *":
        return _nn.Sigmoid_getOutput(self)

    def getInputGrad(self) -> "Vector *":
        return _nn.Sigmoid_getInputGrad(self)

    def getFullOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.Sigmoid_getFullOutputIndices(self)

    def updateOutput(self, input: 'Vector *', output_indices: 'vectori') -> "int":
        return _nn.Sigmoid_updateOutput(self, input, output_indices)

    def updateInputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.Sigmoid_updateInputGrad(self, output_grad)

    def accGradParameters(self, input: 'Vector *', output_grad: 'Vector *', alpha: 'float') -> "int":
        return _nn.Sigmoid_accGradParameters(self, input, output_grad, alpha)
    __swig_destroy__ = _nn.delete_Sigmoid
    __del__ = lambda self: None
Sigmoid_swigregister = _nn.Sigmoid_swigregister
Sigmoid_swigregister(Sigmoid)

class FlexSigmoid(FlexLayer):
    __swig_setmethods__ = {}
    for _s in [FlexLayer]:
        __swig_setmethods__.update(getattr(_s, '__swig_setmethods__', {}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, FlexSigmoid, name, value)
    __swig_getmethods__ = {}
    for _s in [FlexLayer]:
        __swig_getmethods__.update(getattr(_s, '__swig_getmethods__', {}))
    __getattr__ = lambda self, name: _swig_getattr(self, FlexSigmoid, name)
    __repr__ = _swig_repr

    def __init__(self, dim: 'int'):
        this = _nn.new_FlexSigmoid(dim)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def duplicateWithSameWeights(self) -> "FlexLayer *":
        return _nn.FlexSigmoid_duplicateWithSameWeights(self)

    def getInputDim(self) -> "int":
        return _nn.FlexSigmoid_getInputDim(self)

    def getOutputDim(self) -> "int":
        return _nn.FlexSigmoid_getOutputDim(self)

    def inputMustBeSparse(self) -> "bool":
        return _nn.FlexSigmoid_inputMustBeSparse(self)

    def outputMustBeSparse(self) -> "bool":
        return _nn.FlexSigmoid_outputMustBeSparse(self)

    def containsLayers(self) -> "bool":
        return _nn.FlexSigmoid_containsLayers(self)

    def mandatoryIdenticalInputOutputSparsity(self) -> "bool":
        return _nn.FlexSigmoid_mandatoryIdenticalInputOutputSparsity(self)

    def getOutput(self) -> "Vector *":
        return _nn.FlexSigmoid_getOutput(self)

    def getOutputIndices(self) -> "std::vector< int,std::allocator< int > > &":
        return _nn.FlexSigmoid_getOutputIndices(self)

    def getInputGrad(self) -> "Vector *":
        return _nn.FlexSigmoid_getInputGrad(self)

    def setOutputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.FlexSigmoid_setOutputGrad(self, output_grad)

    def getFullOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.FlexSigmoid_getFullOutputIndices(self)

    def updateOutputDenseToDense(self, input: 'Vector *') -> "int":
        return _nn.FlexSigmoid_updateOutputDenseToDense(self, input)

    def updateOutputDenseToWeightedSparse(self, input: 'Vector *', sparse_output: 'vectori') -> "int":
        return _nn.FlexSigmoid_updateOutputDenseToWeightedSparse(self, input, sparse_output)

    def updateOutputWeightedSparseToDense(self, input: 'Vector *', sparse_input: 'vectori') -> "int":
        return _nn.FlexSigmoid_updateOutputWeightedSparseToDense(self, input, sparse_input)

    def updateOutputWeightedSparseToWeightedSparse(self, input: 'Vector *', sparse_input: 'vectori', sparse_output: 'vectori') -> "int":
        return _nn.FlexSigmoid_updateOutputWeightedSparseToWeightedSparse(self, input, sparse_input, sparse_output)

    def updateOutputBinarySparseToDense(self, sparse_input: 'vectori') -> "int":
        return _nn.FlexSigmoid_updateOutputBinarySparseToDense(self, sparse_input)

    def updateOutputBinarySparseToWeightedSparse(self, sparse_input: 'vectori', sparse_output: 'vectori') -> "int":
        return _nn.FlexSigmoid_updateOutputBinarySparseToWeightedSparse(self, sparse_input, sparse_output)

    def backward(self, output_grad: 'Vector *') -> "int":
        return _nn.FlexSigmoid_backward(self, output_grad)

    def updateInputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.FlexSigmoid_updateInputGrad(self, output_grad)

    def accGradParameters(self, alpha: 'float') -> "int":
        return _nn.FlexSigmoid_accGradParameters(self, alpha)
    __swig_destroy__ = _nn.delete_FlexSigmoid
    __del__ = lambda self: None
FlexSigmoid_swigregister = _nn.FlexSigmoid_swigregister
FlexSigmoid_swigregister(FlexSigmoid)

class MSECriterion(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, MSECriterion, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, MSECriterion, name)
    __repr__ = _swig_repr

    def __init__(self):
        this = _nn.new_MSECriterion()
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this
    __swig_setmethods__["grad"] = _nn.MSECriterion_grad_set
    __swig_getmethods__["grad"] = _nn.MSECriterion_grad_get
    if _newclass:
        grad = _swig_property(_nn.MSECriterion_grad_get, _nn.MSECriterion_grad_set)

    def duplicate(self) -> "MSECriterion *":
        return _nn.MSECriterion_duplicate(self)

    def forward(self, *args) -> "float":
        return _nn.MSECriterion_forward(self, *args)

    def backward(self, *args) -> "Vector *":
        return _nn.MSECriterion_backward(self, *args)
    __swig_destroy__ = _nn.delete_MSECriterion
    __del__ = lambda self: None
MSECriterion_swigregister = _nn.MSECriterion_swigregister
MSECriterion_swigregister(MSECriterion)

class Sequential(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, Sequential, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, Sequential, name)
    __repr__ = _swig_repr

    def __init__(self, layers: 'LayerBuilder'):
        this = _nn.new_Sequential(layers)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this
    __swig_setmethods__["layers"] = _nn.Sequential_layers_set
    __swig_getmethods__["layers"] = _nn.Sequential_layers_get
    if _newclass:
        layers = _swig_property(_nn.Sequential_layers_get, _nn.Sequential_layers_set)
    __swig_setmethods__["output"] = _nn.Sequential_output_set
    __swig_getmethods__["output"] = _nn.Sequential_output_get
    if _newclass:
        output = _swig_property(_nn.Sequential_output_get, _nn.Sequential_output_set)

    def duplicateWithSameWeights(self) -> "Sequential *":
        return _nn.Sequential_duplicateWithSameWeights(self)

    def get(self, i: 'int') -> "Layer *":
        return _nn.Sequential_get(self, i)

    def forward(self, input_indices: 'vectori', output_indices: 'vectori') -> "Vector *":
        return _nn.Sequential_forward(self, input_indices, output_indices)

    def backward(self, grad: 'Vector *', output_indices: 'vectori') -> "void":
        return _nn.Sequential_backward(self, grad, output_indices)
    __swig_destroy__ = _nn.delete_Sequential
    __del__ = lambda self: None
Sequential_swigregister = _nn.Sequential_swigregister
Sequential_swigregister(Sequential)

class FlexSequential(FlexLayer):
    __swig_setmethods__ = {}
    for _s in [FlexLayer]:
        __swig_setmethods__.update(getattr(_s, '__swig_setmethods__', {}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, FlexSequential, name, value)
    __swig_getmethods__ = {}
    for _s in [FlexLayer]:
        __swig_getmethods__.update(getattr(_s, '__swig_getmethods__', {}))
    __getattr__ = lambda self, name: _swig_getattr(self, FlexSequential, name)
    __repr__ = _swig_repr
    __swig_setmethods__["layers"] = _nn.FlexSequential_layers_set
    __swig_getmethods__["layers"] = _nn.FlexSequential_layers_get
    if _newclass:
        layers = _swig_property(_nn.FlexSequential_layers_get, _nn.FlexSequential_layers_set)

    def __init__(self, layers: 'FlexLayerBuilder'):
        this = _nn.new_FlexSequential(layers)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def init(self, input_dim: 'int', output_dim: 'int') -> "void":
        return _nn.FlexSequential_init(self, input_dim, output_dim)

    def duplicateWithSameWeights(self) -> "FlexLayer *":
        return _nn.FlexSequential_duplicateWithSameWeights(self)

    def duplicateSequentialWithSameWeights(self) -> "FlexSequential *":
        return _nn.FlexSequential_duplicateSequentialWithSameWeights(self)

    def getLayerIndexToBeginUsingSequenceOutputIndices(self) -> "int":
        return _nn.FlexSequential_getLayerIndexToBeginUsingSequenceOutputIndices(self)

    def get(self, i: 'int') -> "FlexLayer *":
        return _nn.FlexSequential_get(self, i)

    def getInputDim(self) -> "int":
        return _nn.FlexSequential_getInputDim(self)

    def getOutputDim(self) -> "int":
        return _nn.FlexSequential_getOutputDim(self)

    def inputMustBeSparse(self) -> "bool":
        return _nn.FlexSequential_inputMustBeSparse(self)

    def outputMustBeSparse(self) -> "bool":
        return _nn.FlexSequential_outputMustBeSparse(self)

    def containsLayers(self) -> "bool":
        return _nn.FlexSequential_containsLayers(self)

    def mandatoryIdenticalInputOutputSparsity(self) -> "bool":
        return _nn.FlexSequential_mandatoryIdenticalInputOutputSparsity(self)

    def getOutput(self) -> "Vector *":
        return _nn.FlexSequential_getOutput(self)

    def getOutputIndices(self) -> "std::vector< int,std::allocator< int > > &":
        return _nn.FlexSequential_getOutputIndices(self)

    def getInputGrad(self) -> "Vector *":
        return _nn.FlexSequential_getInputGrad(self)

    def setOutputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.FlexSequential_setOutputGrad(self, output_grad)

    def getFullOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.FlexSequential_getFullOutputIndices(self)

    def forward(self, input_indices: 'vectori', output_indices: 'vectori') -> "Vector *":
        return _nn.FlexSequential_forward(self, input_indices, output_indices)

    def updateOutputDenseToDense(self, input: 'Vector *') -> "int":
        return _nn.FlexSequential_updateOutputDenseToDense(self, input)

    def updateOutputDenseToWeightedSparse(self, input: 'Vector *', output_indices: 'vectori') -> "int":
        return _nn.FlexSequential_updateOutputDenseToWeightedSparse(self, input, output_indices)

    def updateOutputWeightedSparseToDense(self, input: 'Vector *', input_indices: 'vectori') -> "int":
        return _nn.FlexSequential_updateOutputWeightedSparseToDense(self, input, input_indices)

    def updateOutputWeightedSparseToWeightedSparse(self, input: 'Vector *', input_indices: 'vectori', output_indices: 'vectori') -> "int":
        return _nn.FlexSequential_updateOutputWeightedSparseToWeightedSparse(self, input, input_indices, output_indices)

    def updateOutputBinarySparseToDense(self, input_indices: 'vectori') -> "int":
        return _nn.FlexSequential_updateOutputBinarySparseToDense(self, input_indices)

    def updateOutputBinarySparseToWeightedSparse(self, input_indices: 'vectori', output_indices: 'vectori') -> "int":
        return _nn.FlexSequential_updateOutputBinarySparseToWeightedSparse(self, input_indices, output_indices)

    def backward(self, output_grad: 'Vector *') -> "int":
        return _nn.FlexSequential_backward(self, output_grad)

    def updateInputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.FlexSequential_updateInputGrad(self, output_grad)

    def accGradParameters(self, alpha: 'float') -> "int":
        return _nn.FlexSequential_accGradParameters(self, alpha)
    __swig_destroy__ = _nn.delete_FlexSequential
    __del__ = lambda self: None
FlexSequential_swigregister = _nn.FlexSequential_swigregister
FlexSequential_swigregister(FlexSequential)

class Sampler(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, Sampler, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, Sampler, name)
    __repr__ = _swig_repr

    def __init__(self, vocab: 'Vocab *', sample_size: 'int'):
        this = _nn.new_Sampler(vocab, sample_size)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this
    __swig_setmethods__["vocab"] = _nn.Sampler_vocab_set
    __swig_getmethods__["vocab"] = _nn.Sampler_vocab_get
    if _newclass:
        vocab = _swig_property(_nn.Sampler_vocab_get, _nn.Sampler_vocab_set)
    __swig_setmethods__["seed"] = _nn.Sampler_seed_set
    __swig_getmethods__["seed"] = _nn.Sampler_seed_get
    if _newclass:
        seed = _swig_property(_nn.Sampler_seed_get, _nn.Sampler_seed_set)
    __swig_setmethods__["sample_size"] = _nn.Sampler_sample_size_set
    __swig_getmethods__["sample_size"] = _nn.Sampler_sample_size_get
    if _newclass:
        sample_size = _swig_property(_nn.Sampler_sample_size_get, _nn.Sampler_sample_size_set)
    __swig_setmethods__["output_size"] = _nn.Sampler_output_size_set
    __swig_getmethods__["output_size"] = _nn.Sampler_output_size_get
    if _newclass:
        output_size = _swig_property(_nn.Sampler_output_size_get, _nn.Sampler_output_size_set)
    __swig_setmethods__["target_values"] = _nn.Sampler_target_values_set
    __swig_getmethods__["target_values"] = _nn.Sampler_target_values_get
    if _newclass:
        target_values = _swig_property(_nn.Sampler_target_values_get, _nn.Sampler_target_values_set)

    def next(self, output_indices: 'vectori') -> "std::vector< int,std::allocator< int > >":
        return _nn.Sampler_next(self, output_indices)

    def getTargetValues(self) -> "Vector *":
        return _nn.Sampler_getTargetValues(self)
    __swig_destroy__ = _nn.delete_Sampler
    __del__ = lambda self: None
Sampler_swigregister = _nn.Sampler_swigregister
Sampler_swigregister(Sampler)

class CBOW(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, CBOW, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, CBOW, name)
    __repr__ = _swig_repr

    def __init__(self, window_indices: 'TrainingExample', vocab: 'Vocab *', sampler: 'Sampler', window_left: 'int', window_right: 'int', model_order: 'bool'):
        this = _nn.new_CBOW(window_indices, vocab, sampler, window_left, window_right, model_order)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this
    __swig_setmethods__["window_indices"] = _nn.CBOW_window_indices_set
    __swig_getmethods__["window_indices"] = _nn.CBOW_window_indices_get
    if _newclass:
        window_indices = _swig_property(_nn.CBOW_window_indices_get, _nn.CBOW_window_indices_set)
    __swig_setmethods__["window_indices_size"] = _nn.CBOW_window_indices_size_set
    __swig_getmethods__["window_indices_size"] = _nn.CBOW_window_indices_size_get
    if _newclass:
        window_indices_size = _swig_property(_nn.CBOW_window_indices_size_get, _nn.CBOW_window_indices_size_set)
    __swig_setmethods__["size"] = _nn.CBOW_size_set
    __swig_getmethods__["size"] = _nn.CBOW_size_get
    if _newclass:
        size = _swig_property(_nn.CBOW_size_get, _nn.CBOW_size_set)
    __swig_setmethods__["input_indices"] = _nn.CBOW_input_indices_set
    __swig_getmethods__["input_indices"] = _nn.CBOW_input_indices_get
    if _newclass:
        input_indices = _swig_property(_nn.CBOW_input_indices_get, _nn.CBOW_input_indices_set)
    __swig_setmethods__["output_indices"] = _nn.CBOW_output_indices_set
    __swig_getmethods__["output_indices"] = _nn.CBOW_output_indices_get
    if _newclass:
        output_indices = _swig_property(_nn.CBOW_output_indices_get, _nn.CBOW_output_indices_set)
    __swig_setmethods__["vocab"] = _nn.CBOW_vocab_set
    __swig_getmethods__["vocab"] = _nn.CBOW_vocab_get
    if _newclass:
        vocab = _swig_property(_nn.CBOW_vocab_get, _nn.CBOW_vocab_set)
    __swig_setmethods__["sampler"] = _nn.CBOW_sampler_set
    __swig_getmethods__["sampler"] = _nn.CBOW_sampler_get
    if _newclass:
        sampler = _swig_property(_nn.CBOW_sampler_get, _nn.CBOW_sampler_set)
    __swig_setmethods__["window_left"] = _nn.CBOW_window_left_set
    __swig_getmethods__["window_left"] = _nn.CBOW_window_left_get
    if _newclass:
        window_left = _swig_property(_nn.CBOW_window_left_get, _nn.CBOW_window_left_set)
    __swig_setmethods__["window_right"] = _nn.CBOW_window_right_set
    __swig_getmethods__["window_right"] = _nn.CBOW_window_right_get
    if _newclass:
        window_right = _swig_property(_nn.CBOW_window_right_get, _nn.CBOW_window_right_set)
    __swig_setmethods__["model_order"] = _nn.CBOW_model_order_set
    __swig_getmethods__["model_order"] = _nn.CBOW_model_order_get
    if _newclass:
        model_order = _swig_property(_nn.CBOW_model_order_get, _nn.CBOW_model_order_set)
    __swig_setmethods__["minimum_input_dimensionality"] = _nn.CBOW_minimum_input_dimensionality_set
    __swig_getmethods__["minimum_input_dimensionality"] = _nn.CBOW_minimum_input_dimensionality_get
    if _newclass:
        minimum_input_dimensionality = _swig_property(_nn.CBOW_minimum_input_dimensionality_get, _nn.CBOW_minimum_input_dimensionality_set)
    __swig_setmethods__["iterator"] = _nn.CBOW_iterator_set
    __swig_getmethods__["iterator"] = _nn.CBOW_iterator_get
    if _newclass:
        iterator = _swig_property(_nn.CBOW_iterator_get, _nn.CBOW_iterator_set)
    __swig_setmethods__["negative"] = _nn.CBOW_negative_set
    __swig_getmethods__["negative"] = _nn.CBOW_negative_get
    if _newclass:
        negative = _swig_property(_nn.CBOW_negative_get, _nn.CBOW_negative_set)
    __swig_setmethods__["starting_win_i"] = _nn.CBOW_starting_win_i_set
    __swig_getmethods__["starting_win_i"] = _nn.CBOW_starting_win_i_get
    if _newclass:
        starting_win_i = _swig_property(_nn.CBOW_starting_win_i_get, _nn.CBOW_starting_win_i_set)
    __swig_setmethods__["win_i"] = _nn.CBOW_win_i_set
    __swig_getmethods__["win_i"] = _nn.CBOW_win_i_get
    if _newclass:
        win_i = _swig_property(_nn.CBOW_win_i_get, _nn.CBOW_win_i_set)
    __swig_setmethods__["pred_i"] = _nn.CBOW_pred_i_set
    __swig_getmethods__["pred_i"] = _nn.CBOW_pred_i_get
    if _newclass:
        pred_i = _swig_property(_nn.CBOW_pred_i_get, _nn.CBOW_pred_i_set)
    __swig_setmethods__["window_len"] = _nn.CBOW_window_len_set
    __swig_getmethods__["window_len"] = _nn.CBOW_window_len_get
    if _newclass:
        window_len = _swig_property(_nn.CBOW_window_len_get, _nn.CBOW_window_len_set)
    __swig_setmethods__["seed"] = _nn.CBOW_seed_set
    __swig_getmethods__["seed"] = _nn.CBOW_seed_get
    if _newclass:
        seed = _swig_property(_nn.CBOW_seed_get, _nn.CBOW_seed_set)
    __swig_setmethods__["has_next"] = _nn.CBOW_has_next_set
    __swig_getmethods__["has_next"] = _nn.CBOW_has_next_get
    if _newclass:
        has_next = _swig_property(_nn.CBOW_has_next_get, _nn.CBOW_has_next_set)

    def next(self) -> "void":
        return _nn.CBOW_next(self)

    def reset(self) -> "void":
        return _nn.CBOW_reset(self)

    def getCopyForSection(self, starting: 'int', ending: 'int') -> "CBOW *":
        return _nn.CBOW_getCopyForSection(self, starting, ending)

    def getInputIndicesReference(self) -> "std::vector< int,std::allocator< int > > &":
        return _nn.CBOW_getInputIndicesReference(self)

    def getOutputIndicesReference(self) -> "std::vector< int,std::allocator< int > > &":
        return _nn.CBOW_getOutputIndicesReference(self)

    def getTargetValuesReference(self) -> "Vector *":
        return _nn.CBOW_getTargetValuesReference(self)
    __swig_destroy__ = _nn.delete_CBOW
    __del__ = lambda self: None
CBOW_swigregister = _nn.CBOW_swigregister
CBOW_swigregister(CBOW)

class StochasticGradientWorker(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, StochasticGradientWorker, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, StochasticGradientWorker, name)
    __repr__ = _swig_repr

    def __init__(self, mlp: 'FlexSequential', criterion: 'MSECriterion', training_generator: 'CBOW', alpha: 'float', iterations: 'int', worker_id: 'int', num_workers: 'int'):
        this = _nn.new_StochasticGradientWorker(mlp, criterion, training_generator, alpha, iterations, worker_id, num_workers)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this
    __swig_setmethods__["mlp"] = _nn.StochasticGradientWorker_mlp_set
    __swig_getmethods__["mlp"] = _nn.StochasticGradientWorker_mlp_get
    if _newclass:
        mlp = _swig_property(_nn.StochasticGradientWorker_mlp_get, _nn.StochasticGradientWorker_mlp_set)
    __swig_setmethods__["criterion"] = _nn.StochasticGradientWorker_criterion_set
    __swig_getmethods__["criterion"] = _nn.StochasticGradientWorker_criterion_get
    if _newclass:
        criterion = _swig_property(_nn.StochasticGradientWorker_criterion_get, _nn.StochasticGradientWorker_criterion_set)
    __swig_setmethods__["training_generator"] = _nn.StochasticGradientWorker_training_generator_set
    __swig_getmethods__["training_generator"] = _nn.StochasticGradientWorker_training_generator_get
    if _newclass:
        training_generator = _swig_property(_nn.StochasticGradientWorker_training_generator_get, _nn.StochasticGradientWorker_training_generator_set)
    __swig_setmethods__["alpha"] = _nn.StochasticGradientWorker_alpha_set
    __swig_getmethods__["alpha"] = _nn.StochasticGradientWorker_alpha_get
    if _newclass:
        alpha = _swig_property(_nn.StochasticGradientWorker_alpha_get, _nn.StochasticGradientWorker_alpha_set)
    __swig_setmethods__["iterations"] = _nn.StochasticGradientWorker_iterations_set
    __swig_getmethods__["iterations"] = _nn.StochasticGradientWorker_iterations_get
    if _newclass:
        iterations = _swig_property(_nn.StochasticGradientWorker_iterations_get, _nn.StochasticGradientWorker_iterations_set)
    __swig_setmethods__["worker_id"] = _nn.StochasticGradientWorker_worker_id_set
    __swig_getmethods__["worker_id"] = _nn.StochasticGradientWorker_worker_id_get
    if _newclass:
        worker_id = _swig_property(_nn.StochasticGradientWorker_worker_id_get, _nn.StochasticGradientWorker_worker_id_set)
    __swig_setmethods__["num_workers"] = _nn.StochasticGradientWorker_num_workers_set
    __swig_getmethods__["num_workers"] = _nn.StochasticGradientWorker_num_workers_get
    if _newclass:
        num_workers = _swig_property(_nn.StochasticGradientWorker_num_workers_get, _nn.StochasticGradientWorker_num_workers_set)
    __swig_destroy__ = _nn.delete_StochasticGradientWorker
    __del__ = lambda self: None
StochasticGradientWorker_swigregister = _nn.StochasticGradientWorker_swigregister
StochasticGradientWorker_swigregister(StochasticGradientWorker)

class StochasticGradient(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, StochasticGradient, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, StochasticGradient, name)
    __repr__ = _swig_repr

    def __init__(self, mlp: 'FlexSequential', criterion: 'MSECriterion'):
        this = _nn.new_StochasticGradient(mlp, criterion)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this
    __swig_setmethods__["mlp"] = _nn.StochasticGradient_mlp_set
    __swig_getmethods__["mlp"] = _nn.StochasticGradient_mlp_get
    if _newclass:
        mlp = _swig_property(_nn.StochasticGradient_mlp_get, _nn.StochasticGradient_mlp_set)
    __swig_setmethods__["criterion"] = _nn.StochasticGradient_criterion_set
    __swig_getmethods__["criterion"] = _nn.StochasticGradient_criterion_get
    if _newclass:
        criterion = _swig_property(_nn.StochasticGradient_criterion_get, _nn.StochasticGradient_criterion_set)

    def train(self, training_generator: 'CBOW', alpha: 'float', iterations: 'int', num_threads: 'int') -> "float":
        return _nn.StochasticGradient_train(self, training_generator, alpha, iterations, num_threads)
    __swig_destroy__ = _nn.delete_StochasticGradient
    __del__ = lambda self: None
StochasticGradient_swigregister = _nn.StochasticGradient_swigregister
StochasticGradient_swigregister(StochasticGradient)


def TrainModelThread(sgd: 'void *') -> "void *":
    return _nn.TrainModelThread(sgd)
TrainModelThread = _nn.TrainModelThread
class LinearTree(Layer):
    __swig_setmethods__ = {}
    for _s in [Layer]:
        __swig_setmethods__.update(getattr(_s, '__swig_setmethods__', {}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, LinearTree, name, value)
    __swig_getmethods__ = {}
    for _s in [Layer]:
        __swig_getmethods__.update(getattr(_s, '__swig_getmethods__', {}))
    __getattr__ = lambda self, name: _swig_getattr(self, LinearTree, name)
    __repr__ = _swig_repr

    def __init__(self, input_dim: 'int', output_dim: 'int'):
        this = _nn.new_LinearTree(input_dim, output_dim)
        try:
            self.this.append(this)
        except __builtin__.Exception:
            self.this = this

    def init(self, input_dim: 'int', output_dim: 'int') -> "void":
        return _nn.LinearTree_init(self, input_dim, output_dim)
    __swig_setmethods__["tree"] = _nn.LinearTree_tree_set
    __swig_getmethods__["tree"] = _nn.LinearTree_tree_get
    if _newclass:
        tree = _swig_property(_nn.LinearTree_tree_get, _nn.LinearTree_tree_set)
    __swig_setmethods__["paths"] = _nn.LinearTree_paths_set
    __swig_getmethods__["paths"] = _nn.LinearTree_paths_get
    if _newclass:
        paths = _swig_property(_nn.LinearTree_paths_get, _nn.LinearTree_paths_set)
    __swig_setmethods__["codes"] = _nn.LinearTree_codes_set
    __swig_getmethods__["codes"] = _nn.LinearTree_codes_get
    if _newclass:
        codes = _swig_property(_nn.LinearTree_codes_get, _nn.LinearTree_codes_set)
    __swig_setmethods__["factored_output"] = _nn.LinearTree_factored_output_set
    __swig_getmethods__["factored_output"] = _nn.LinearTree_factored_output_get
    if _newclass:
        factored_output = _swig_property(_nn.LinearTree_factored_output_get, _nn.LinearTree_factored_output_set)
    __swig_setmethods__["heap"] = _nn.LinearTree_heap_set
    __swig_getmethods__["heap"] = _nn.LinearTree_heap_get
    if _newclass:
        heap = _swig_property(_nn.LinearTree_heap_get, _nn.LinearTree_heap_set)
    __swig_setmethods__["weights"] = _nn.LinearTree_weights_set
    __swig_getmethods__["weights"] = _nn.LinearTree_weights_get
    if _newclass:
        weights = _swig_property(_nn.LinearTree_weights_get, _nn.LinearTree_weights_set)

    def sigmoid(self, x: 'float') -> "float":
        return _nn.LinearTree_sigmoid(self, x)

    def createBinaryTree(self) -> "void":
        return _nn.LinearTree_createBinaryTree(self)

    def getPathSize(self, i: 'int') -> "int":
        return _nn.LinearTree_getPathSize(self, i)

    def getCodeSize(self, i: 'int') -> "int":
        return _nn.LinearTree_getCodeSize(self, i)

    def getPath(self, i: 'int', j: 'int') -> "int":
        return _nn.LinearTree_getPath(self, i, j)

    def getCode(self, i: 'int', j: 'int') -> "bool":
        return _nn.LinearTree_getCode(self, i, j)

    def duplicateWithSameWeights(self) -> "Layer *":
        return _nn.LinearTree_duplicateWithSameWeights(self)

    def getInputDim(self) -> "int":
        return _nn.LinearTree_getInputDim(self)

    def getOutputDim(self) -> "int":
        return _nn.LinearTree_getOutputDim(self)

    def hasSparseInput(self) -> "bool":
        return _nn.LinearTree_hasSparseInput(self)

    def hasSparseOutput(self) -> "bool":
        return _nn.LinearTree_hasSparseOutput(self)

    def getOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.LinearTree_getOutputIndices(self)

    def getOutput(self) -> "Vector *":
        return _nn.LinearTree_getOutput(self)

    def getInputGrad(self) -> "Vector *":
        return _nn.LinearTree_getInputGrad(self)

    def getFullOutputIndices(self) -> "std::vector< int,std::allocator< int > >":
        return _nn.LinearTree_getFullOutputIndices(self)

    def dfs(self, arg2: 'int32_t', arg3: 'int32_t', arg4: 'float', arg5: 'std::vector< std::pair< float,int32_t >,std::allocator< std::pair< float,int32_t > > > &', arg6: 'Vector *', arg7: 'int') -> "void":
        return _nn.LinearTree_dfs(self, arg2, arg3, arg4, arg5, arg6, arg7)

    def predict(self, input: 'Vector *', output_indices: 'vectori') -> "int":
        return _nn.LinearTree_predict(self, input, output_indices)

    def updateOutput(self, input: 'Vector *', output_indices: 'vectori') -> "int":
        return _nn.LinearTree_updateOutput(self, input, output_indices)

    def updateInputGrad(self, output_grad: 'Vector *') -> "int":
        return _nn.LinearTree_updateInputGrad(self, output_grad)

    def accGradParameters(self, input: 'Vector *', output_grad: 'Vector *', alpha: 'float') -> "int":
        return _nn.LinearTree_accGradParameters(self, input, output_grad, alpha)
    __swig_destroy__ = _nn.delete_LinearTree
    __del__ = lambda self: None
LinearTree_swigregister = _nn.LinearTree_swigregister
LinearTree_swigregister(LinearTree)

# This file is compatible with both classic and new-style classes.


